# 4. 流匹配
给定一个源分布\(p\)和一个目标分布\(q\)，流匹配（Flow Matching，FM）（Lipman等人，2022；Liu等人，2022；Albergo和Vanden-Eijnden，2022）是一种用于训练流模型的可扩展方法。该流模型由可学习的速度场\(u_{t}^{\theta}\)定义，其旨在解决流匹配问题：
\[ 找到生成概率路径 p_{t} 的 u_{t}^{\theta}，其中 p_{0}=p 且 p_{1}=q \tag{4.1}\]

在上述方程中，“生成”的含义与方程（3.24）中的一致。
流匹配框架经过以下流程（a）确定一个已知的源分布\(p\)和一个未知的数据目标分布\(q\)；（b）设定一条从\(p_{0}=p\)插值到\(p_{1}=q\)的概率路径\(p_{t}\)；（c）学习一个由神经网络实现的速度场\(u_{t}^{\theta}\)，该速度场生成上述概率路径\(p_{t}\)；（d）通过求解带有\(u_t^\theta\)的常微分方程（ODE），从学习到的模型中进行采样。为了在步骤（c）中学习速度场\(u_t^\theta\)，流匹配（FM）会最小化回归损失：

\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{X_{t} \sim p_{t}} D\left(u_{t}\left(X_{t}\right), u_{t}^{\theta}\left(X_{t}\right)\right) \tag{4.2}
\]

其中，\(D\)是向量之间的不相似度度量，例如平方\(\ell_{2}\)范数\(D(u, v) = \|u - v\|^{2}\)。直观而言，流匹配损失促使我们的可学习速度场\(u_{t}^{\theta}\)与已知能生成期望概率路径\(p_{t}\)的真实速度场\(u_{t}\)相匹配。下面我们开始阐述流匹配，首先介绍如何构建\(p_{t}\)和\(u_{t}\)，以及损失（4.2）的实际实现方式。

## 4.1 数据
需要重申的是，设源样本为随机变量\(X_{0} \sim p\)，目标样本为随机变量\(X_{1} \sim q\)。通常，源样本遵循一种已知且易于采样的分布，而目标样本则以有限大小的数据集形式呈现。根据具体应用场景，目标样本可以是图像、视频、音频片段，或者其他类型的高维、结构丰富的数据。源样本和目标样本可以是相互独立的，也可以来自一种被称为“耦合”的一般联合分布：
\[
\left(X_{0}, X_{1}\right) \sim \pi_{0,1}\left(X_{0}, X_{1}\right) \tag{4.3}
\]
其中，若不存在已知的耦合关系，源-目标样本则遵循独立耦合\(\pi_{0,1}(X_{0}, X_{1}) = p(X_{0}) q(X_{1})\)。独立的源-目标分布的一个常见例子是，从随机高斯噪声向量\(X_{0} \sim N(0, I)\)生成图像\(X_{1}\)。作为依赖耦合的例子，可以考虑从低分辨率版本\(x_{0}\)生成高分辨率图像\(X_{1}\)，或者从灰度视频\(x_{0}\)生成彩色视频\(X_{1}\)。

## 4.2 建立概率路径
流匹配通过采用条件策略，极大地简化了设计概率路径\(p_t\)及其相应速度场\(u_t\)的问题。首先举个例子，考虑将\(p_t\)的设计以单个目标样本\(X_1 = x_1\)为条件，得到条件概率路径\(p_{t|1}(x|x_1)\)。然后，我们可以通过聚合此类条件概率路径\(p_{t|1}\)来构建整体的边际概率路径\(p_t\)：
\[
p_t(x)=\int p_{t|1}(x|x_1)q(x_1)dx_1 \tag{4.4}
\]
为了解决流匹配问题，我们希望\(p_t\)满足以下边界条件：
\[
p_0 = p,\ p_1=q \tag{4.5}
\]
也就是说，边际概率路径\(p_t\)在时间\(t=0\)时从源分布\(p\)插值到时间\(t=1\)时的目标分布\(q\)。这些边界条件可以通过要求条件概率路径满足以下来强制执行。

\[p_{0 | 1}\left(x | x_{1}\right)=\pi_{0 | 1}\left(x | x_{1}\right), and p_{1 | 1}\left(x | x_{1}\right)=\delta_{x_{1}}(x), \tag{4.6}\]

其中，条件耦合\(\pi_{0|1}(x_0|x_1)=\pi_{0,1}(x_0,x_1)/q(x_1)\)，\(\delta_{x_1}\)是以\(x_1\)为中心的狄拉克测度。对于独立耦合\(\pi_{0,1}(x_0,x_1)=p(x_0)q(x_1)\)，上述第一个约束简化为\(p_{0|1}(x|x_1)=p(x)\)。由于狄拉克测度没有密度，第二个约束应理解为：对于连续函数\(f\)，当\(t\to1\)时，\(\int p_{t|1}(x|y)f(y)dy\to f(x)\)。注意，将式（4.6）代入式（4.4）可以验证边界条件（4.5）。

式（2.2）给出了一个满足式（4.6）中条件的条件概率路径的常见示例：当\(t\to1\)时，\(\mathcal{N}(\cdot|tx_1,(1-t)^2I)\to\delta_{x_1}(\cdot)\)。

## 4.3 推导生成速度场

有了边际概率路径\(p_{t}\)，我们现在来构建生成\(p_{t}\)的速度场\(u_{t}\)。生成速度场\(u_{t}\)是多个条件速度场\(u_{t}(x | x_{1})\)的平均值，且满足：
\[u_{t}\left(\cdot | x_{1}\right)生成p_{t | 1}\left(\cdot | x_{1}\right) \tag{4.7}\]

那么，生成边际路径\(p_{t}(x)\)的边际速度场\(u_{t}(x)\)（如图3d所示）可通过对目标样本的条件速度场\(u_{t}(x | x_{1})\)求平均得到：
\[
u_{t}(x)=\int u_{t}\left(x | x_{1}\right) p_{1 | t}\left(x_{1} | x\right) d x_{1} \tag{4.8}
\]

为了用已知术语表示上述方程，回顾贝叶斯法则：
\[
p_{1 | t}\left(x_{1} | x\right)=\frac{p_{t | 1}\left(x | x_{1}\right) q\left(x_{1}\right)}{p_{t}(x)}, \tag{4.9}
\]

该式对所有满足\(p_{t}(x) > 0\)的\(x\)都有定义。式（4.8）可理解为条件速度\(u_{t}(x | x_{1})\)的加权平均，其中权重\(p_{1 | t}(x_{1} | x)\)表示在给定当前样本\(x\)的情况下，目标样本\(x_{1}\)的后验概率。

式（4.8）还可以通过条件期望来解释（见3.2节）。即，如果\(X_{t}\)是任意随机变量，且满足\(X_{t} \sim p_{t | 1}(\cdot | X_{1})\)，或者等价地，\((X_{t}, X_{1})\)的联合分布具有密度\(p_{t, 1}(x, x_{1}) = p_{t | 1}(x | x_{1}) q(x_{1})\)，那么利用式（3.12）将式（4.8）写成条件期望的形式，我们可得：
\[
u_{t}(x) = \mathbb{E}\left[u_{t}\left(X_{t} | X_{1}\right) | X_{t} = x\right] \tag{4.10}
\]

这就给出了一个有用的解释：\(u_{t}(x)\)是在给定\(X_{t} = x\)时，对\(u_{t}(X_{t} | X_{1})\)的最小二乘近似（见3.2节）。需要注意的是，式（4.10）中的\(x_{t}\)通常与由最终流模型（3.16）定义的\(X_{t}\)是不同的随机变量，尽管它们具有相同的边际概率\(p_{t}(x)\)。

## 4.4 一般的条件化与边缘化技巧

为证明上述构造的合理性，我们需在弱假设条件下证明：由式（4.8）和式（4.10）得到的边际速度场\(u_{t}\)，能够生成由式（4.4）给出的边际概率路径\(p_{t}\)。用于证明这一结论的数学工具是质量守恒定理（定理2）。接下来，我们考虑一个略具一般性的设定，该设定在本文后续内容中会有所帮助。具体而言，通过条件\(X_{1}=x_{1}\)构建条件概率路径与速度场的做法并无特殊之处。正如Tong等人（2023）所指出的，前一节的分析可推广到以任意随机变量\(Z \in \mathbb{R}^{m}\)（其概率密度函数为\(p_{Z}\)）为条件的情形。由此可得到边际概率路径：
\[p_{t}(x)=\int p_{t | Z}(x | z) p_{Z}(z) d z,\tag{4.11}\]

而生成该边际概率路径的边际速度场为：
\[u_{t}(x)=\int u_{t}(x | z) p_{Z | t}(z | x) d z=\mathbb{E}\left[u_{t}\left(X_{t} | Z\right) | X_{t}=x\right],\tag{4.12}\]

其中，\(u_t(\cdot|z)\)生成\(p_{t|Z}(\cdot|z)\)；在\(p_t(x) > 0\)的条件下，由贝叶斯法则可得\(p_{Z|t}(z|x) = \frac{p_{t|Z}(x|z)p_Z(z)}{p_t(x)}\)；且\(X_t \sim p_{t|Z}(\cdot|Z)\)。显然，令\(Z = X_1\)即可还原前几节中的构造。在证明主要结果之前，我们需要先给出一些正则性假设，具体如下：

**假设1**:\(p_{t | Z}(x | z)\) 作为 \((t, x)\) 的函数，属于 \(C^{1}([0,1) \times \mathbb{R}^{d})\) 类（即关于 \((t, x)\) 一阶连续可微）；\(u_{t}(x | z)\) 作为 \((t, x)\) 的函数，属于 \(C^{1}([0,1) \times \mathbb{R}^{d}, \mathbb{R}^{d})\) 类（即关于 \((t, x)\) 一阶连续可微，且取值于 \(\mathbb{R}^{d}\)）。此外，\(p_{Z}\) 具有有界支撑集，也就是说，在 \(\mathbb{R}^{m}\) 中某一有界集合之外，\(p_{Z}(x) = 0\)。最后，对于所有 \(x \in \mathbb{R}^{d}\) 和 \(t \in [0,1)\)，均有 \(p_{t}(x) > 0\)。

这些均为弱假设。例如，可通过找到满足\(p_{Z}(z)>0\)且\(p_{t | Z}(\cdot | z)>0\)的条件\(z\)，来证明\(p_{t}(x)>0\)。在实际应用中，对于任意小的\(\epsilon>0\)，可通过考虑\((1-(1-t) \epsilon) p_{t | Z}+(1-t) \epsilon \mathcal{N}(0, I)\)来满足该条件。满足此假设的\(p_{t | Z}(\cdot | z)\)实例之一是式（2.2）中的路径，其中我们令\(Z=X_{1}\)。现在，我们可以给出主要结果：

**定理3（边际化技巧）**：在假设1的条件下，若\(u_{t}(x | z)\)满足条件可积性，且能生成条件概率路径\(p_{t}(\cdot | z)\)，则对于所有\(t \in [0,1)\)，边际速度场\(u_{t}\)能生成边际概率路径\(p_{t}\)。

在上述定理中，“条件可积”指的是质量守恒定理（式3.26）中可积性条件的条件形式，即：
\[\int_{0}^{1} \iint\left\| u_{t}(x | z)\right\| p_{t | Z}(x | z) p_{Z}(x) d z d x d t<\infty .\tag{4.13}\]

证明：该结论可通过验证定理2中质量守恒定理的两个条件得出。首先，我们验证\((u_{t}, p_{t})\)是否满足连续性方程（式3.25）。由于\(u_{t}(\cdot | x_{1})\)生成\(p_{t}(\cdot | x_{1})\)，因此我们有


\[\frac{d}{d t} p_{t}(x) \stackrel{(i)}{=} \int \frac{d}{d t} p_{t | Z}(x | z) p_{Z}(x) d z \tag{4.14}\]

\[\stackrel{(i i)}{=}-\int div_{x}\left[u_{t}(x | z) p_{t | Z}(x | z)\right] p_{Z}(z) d z\tag{4.15}\]

\[\stackrel{(i)}{=}-div_{x} \int u_{t}(x | z) p_{t | Z}(x | z) p_{Z}(z) d z\tag{4.16}\]

\[\stackrel{( iii )}{=}-div_{x}\left[u_{t}(x) p_{t}(x)\right] . \tag{4.17}\]

等式（i）的成立源于交换微分运算（分别为\(\frac{d}{dt}\)和\(\text{div}_x\)）与积分运算的顺序，其合理性可由以下因素证明：莱布尼茨法则（Leibniz’s rule）、\(p_{t|Z}(x|z)\)与\(u_t(x|z)\)关于\(t\)和\(x\)是一阶连续可微（\(C^1\)）函数的性质，以及\(p_Z\)具有有界支撑集的性质（因此所有被积函数作为有界集合上的连续函数均满足可积性）。等式（ii）的成立源于\(u_t(\cdot|z)\)生成\(p_{t|Z}(\cdot|z)\)这一事实以及定理2。等式（iii）的成立源于对\(p_t(x)\)（由假设可知其严格为正）进行同乘同除运算，并使用\(u_t\)的表达式（式4.12）。

为验证定理2中的第二个（也是最后一个）条件，我们需证明\(u_{t}\)满足可积性与局部 Lipschitz 连续性。由于一阶连续可微（\(C^1\)）函数必为局部 Lipschitz 连续函数，因此只需验证对所有\((t, x)\)，\(u_{t}(x)\)均属于\(C^1\)类即可。这一结论可通过验证\(u_{t}(x | z)\)与\(p_{t|Z}(x | z)\)均为\(C^1\)类函数且\(p_{t}(x)>0\)来推导，而这些条件均由假设成立。此外，\(u_{t}(x)\)的可积性可由\(u_{t}(x | z)\)的条件可积性证明：
\[\int_{0}^{1} \int\left\| u_{t}(x)\right\| p_{t}(x) d x d t \leq \int_{0}^{1} \iint\left\| u_{t}(x | z)\right\| p_{t | Z}(x | z) p_{Z}(z) d z d x d t<\infty, \tag{4.18}\]
其中第一个不等式由向量形式的 Jensen 不等式推导得出。■

## 4.5 流匹配损失函数
在确定目标速度场\(u_t\)能生成从分布\(p\)到分布\(q\)的给定概率路径\(p_t\)之后，目前缺少的关键要素是一个易于处理的损失函数——该函数需用于学习速度场模型\(u_t^\theta\)，使其尽可能接近目标速度场\(u_t\)。直接构建该损失函数的主要障碍在于，计算目标速度场\(u_t\)是不可行的，因为这需要对整个训练集进行边际化运算（即对式（4.8）中的\(x_1\)积分，或对式（4.12）中的\(z\)积分）。幸运的是，一类被称为布雷格曼散度（Bregman divergences）的损失函数可提供无偏梯度，仅通过条件速度场\(u_t(x|z)\)就能实现对\(u_t^\theta(x)\)的学习。

布雷格曼散度（Bregman divergences）用于衡量两个向量\(u, v \in \mathbb{R}^d\)之间的差异，其定义为：
\[D(u, v):=\Phi(u)-\left[\Phi(v)+\langle u-v, \nabla \Phi(v)\rangle\right], \quad\tag{4.19}\]

其中，\(\Phi: \mathbb{R}^d \to \mathbb{R}\)是定义在凸集\(\Omega \subset \mathbb{R}^d\)上的严格凸函数。布雷格曼散度衡量的是\(\Phi(u)\)与\(\Phi\)在\(v\)附近展开的线性近似（该近似在\(u\)处取值）之间的差值。由于线性近似是凸函数的全局下界，因此有\(D(u, v) \geq 0\)。此外，因\(\Phi\)是严格凸函数，可得\(D(u, v)=0\)当且仅当\(u=v\)。最基础的布雷格曼散度是欧几里得平方距离\(D(u, v)=\|u-v\|^2\)，这是通过选取\(\Phi(u)=\|u\|^2\)得到的。

布雷格曼散度之所以对流匹配（Flow Matching）有用，关键在于其关于第二个参数的梯度具有仿射不变性（Holderrieth等人，2024）：
\[\nabla_{v} D\left(a u_{1}+b u_{2}, v\right)=a \nabla_{v} D\left(u_{1}, v\right)+b \nabla_{v} D\left(u_{2}, v\right) \quad (\forall a+b=1), \tag{4.20}\]
这一点可通过式（4.19）验证。借助仿射不变性，我们可实现期望与梯度的互换，具体如下：
\[\nabla_{v} D(\mathbb{E}[Y], v)=\mathbb{E}\left[\nabla_{v} D(Y, v)\right] \quad (\forall \text{随机变量 } Y \in \mathbb{R}^d). \tag{4.21}\]

流匹配（Flow Matching，FM）损失采用布雷格曼散度（Bregman divergence），在概率路径\(p_t\)上实现可学习速度场\(u_t^\theta(x)\)对目标速度场\(u_t(x)\)的回归，其定义为：
\[
\mathcal{L}_{FM}(\theta)=\mathbb{E}_{t, X_{t} \sim p_{t}} D\left(u_{t}\left(X_{t}\right), u_{t}^{\theta}\left(X_{t}\right)\right),
\]
其中时间\(t \sim \mathcal{U}[0,1]\)（即\(t\)服从区间\([0,1]\)上的均匀分布）。然而，如前文所述，目标速度场\(u_t\)难以直接计算，因此上述损失无法按原式直接求解。取而代之，我们考虑更简洁且易于计算的条件流匹配（Conditional Flow Matching，CFM）损失：
\[
\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t, Z, X_{t} \sim p_{t | Z}(\cdot | Z)} D\left(u_{t}\left(X_{t} | Z\right), u_{t}^{\theta}\left(X_{t}\right)\right) . \tag{4.23}
\]

从学习角度而言，这两种损失是等价的，因为它们的梯度完全一致（Holderrieth等人，2024）：

**定理4**
流匹配损失（Flow Matching loss）与条件流匹配损失（Conditional Flow Matching loss）的梯度一致：
\[
\nabla_{\theta} \mathcal{L}_{FM}(\theta) = \nabla_{\theta} \mathcal{L}_{CFM}(\theta). \tag{4.24}
\]
具体而言，条件流匹配损失的极小值点即为边际速度场\(u_t(x)\)。

证明：
通过直接计算可完成证明：
\[
\begin{aligned} 
\nabla_{\theta} \mathcal{L}_{FM}(\theta) & =\nabla_{\theta} \mathbb{E}_{t, X_{t} \sim p_{t}} D\left(u_{t}\left(X_{t}\right), u_{t}^{\theta}\left(X_{t}\right)\right) \\ 
& =\mathbb{E}_{t, X_{t} \sim p_{t}} \nabla_{\theta} D\left(u_{t}\left(X_{t}\right), u_{t}^{\theta}\left(X_{t}\right)\right) \\ 
& \stackrel{(i)}{=} \mathbb{E}_{t, X_{t} \sim p_{t}} \nabla_{v} D\left(u_{t}\left(X_{t}\right), u_{t}^{\theta}\left(X_{t}\right)\right) \nabla_{\theta} u_{t}^{\theta}\left(X_{t}\right) \\ 
& \stackrel{(4.12)}{=} \mathbb{E}_{t, X_{t} \sim p_{t}} \nabla_{v} D\left(\mathbb{E}_{Z \sim p_{Z | t}\left(\cdot | X_{t}\right)}\left[u_{t}\left(X_{t} | Z\right)\right], u_{t}^{\theta}\left(X_{t}\right)\right) \nabla_{\theta} u_{t}^{\theta}\left(X_{t}\right) \\ 
& \stackrel{(ii)}{=} \mathbb{E}_{t, X_{t} \sim p_{t}} \mathbb{E}_{Z \sim p_{Z | t}\left(\cdot | X_{t}\right)}\left[\nabla_{v} D\left(u_{t}\left(X_{t} | Z\right), u_{t}^{\theta}\left(X_{t}\right)\right)\right] \nabla_{\theta} u_{t}^{\theta}\left(X_{t}\right) \\
& \stackrel{(iii)}{=} \mathbb{E}_{t, X_{t} \sim p_{t}, Z \sim p_{Z | t}\left(\cdot | X_{t}\right)} \nabla_{v} D\left(u_{t}\left(X_{t} | Z\right), u_{t}^{\theta}\left(X_{t}\right)\right) \nabla_{\theta} u_{t}^{\theta}\left(X_{t}\right) \\
& \stackrel{(iv)}{=} \mathbb{E}_{t, Z, X_{t} \sim p_{t | Z}\left(\cdot | Z\right)} \nabla_{v} D\left(u_{t}\left(X_{t} | Z\right), u_{t}^{\theta}\left(X_{t}\right)\right) \nabla_{\theta} u_{t}^{\theta}\left(X_{t}\right) \\
& =\nabla_{\theta} \mathcal{L}_{CFM}(\theta)
\end{aligned}
\]
其中，(i)和(iii)处运用了链式法则；(ii)处是对\(X_t\)施加条件后，应用式(4.21)所得结果；(iv)处则运用了贝叶斯法则。■

用于学习条件期望的布雷格曼散度：定理4是一个更具一般性结论的特殊实例，该一般性结论利用布雷格曼散度来学习条件期望，具体描述如下。此结论将在全文中被多次使用，并为流匹配（Flow Matching）背后所有可扩展损失函数提供理论基础：

**命题1（用于学习条件期望的布雷格曼散度）**
设\(X \in S_{X}\)、\(Y \in S_{Y}\)为定义在状态空间\(S_{X}\)、\(S_{Y}\)上的随机变量，\(g: \mathbb{R}^{p} \times S_{X} \to \mathbb{R}^{n}\)为映射函数，满足\((\theta, x) \mapsto g^{\theta}(x)\)，其中\(\theta \in \mathbb{R}^{p}\)表示可学习参数。设\(D_{x}(u, v)\)（\(x \in S_{X}\)）为定义在凸集\(\Omega \subset \mathbb{R}^{n}\)上的布雷格曼散度，且该凸集包含函数\(f\)的像集。则有：
\[
\nabla_{\theta} \mathbb{E}_{X, Y} D_{X}\left(Y, g^{\theta}(X)\right)=\nabla_{\theta} \mathbb{E}_{X} D_{X}\left(\mathbb{E}[Y | X], g^{\theta}(X)\right)
\tag{4.25}
\]

特别地，对于所有满足\(p_{X}(x)>0\)的\(x\)，\(g^{\theta}(x)\)关于\(\theta\)的全局最小值满足：
\[g^{\theta}(x)=\mathbb{E}[Y | X=x] \tag{4.26}\]

证明：假设\(g^{\theta}\)关于\(\theta\)可微，且\(X\)和\(Y\)的分布、\(D_{x}\)以及\(g\)满足微分与积分可交换的条件，则有：
\[
\begin{aligned} 
\nabla_{\theta} \mathbb{E}_{X, Y} D_{X}\left(Y, g^{\theta}(X)\right) & \stackrel{(i)}{=} \mathbb{E}_{X}\left[\mathbb{E}\left[\nabla_{v} D_{X}\left(Y, g^{\theta}(X)\right) \nabla_{\theta} g^{\theta}(X) | X\right]\right] \\ 
& \stackrel{(i i)}{=} \mathbb{E}_{X}\left[\nabla_{v} D_{X}\left(\mathbb{E}[Y | X], g^{\theta}(X)\right) \nabla_{\theta} g^{\theta}(X)\right] \\ 
& \stackrel{(i i i)}{=} \mathbb{E}_{X}\left[\nabla_{\theta} D_{X}\left(\mathbb{E}[Y | X], g^{\theta}(X)\right)\right] \\ 
& =\nabla_{\theta} \mathbb{E}_{X} D_{X}\left(\mathbb{E}[Y | X], g^{\theta}(X)\right), 
\end{aligned}
\]
其中（i）由链式法则和期望的全概率公式（3.11）得到。等式（ii）由（4.21）得到。等式（iii）再次使用链式法则。最后，对于每个满足\(p_{X}(x)>0\)的\(x \in S_{X}\)，我们可以选择\(g^{\theta}(x)=\mathbb{E}[Y | X=x]\)，从而得到\(\mathbb{E}_{X} D_{X}(\mathbb{E}[Y | X], g^{\theta}(X))=0\)，这必定是关于\(\theta\)的全局最小值。■

通过令\(X=X_{t}\)、\(Y=u_{t}(X_{t} | Z)\)、\(g^{\theta}(x)=u_{t}^{\theta}(x)\)，并对\(t \sim U[0,1]\)取期望，即可由命题1直接推出定理4。

**一般时间分布**：Flow Matching（FM）损失的一个实用变体是从非均匀分布中采样时间\(t\)。具体而言，考虑\(t \sim \omega(t)\)，其中\(\omega\)是定义在区间\([0, 1]\)上的概率密度函数（PDF）。这会得到以下加权目标函数：
\[
\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t \sim \omega, Z, X_{t}} D\left(u_{t}\left(X_{t} | Z\right), u_{t}^{\theta}\left(X_{t}\right)\right)=\mathbb{E}_{t \sim U, Z, X_{t}} \omega(t) D\left(u_{t}\left(X_{t} | Z\right), u_{t}^{\theta}\left(X_{t}\right)\right)
\]

尽管在数学上是等价的，但在大规模图像生成任务中，从分布\(t \sim \omega\)中采样时间\(t\)，其性能要优于直接使用权重\(\omega(t)\)（Esser 等人，2024）。

## 4.6 利用条件流解决条件生成问题
截至目前，我们已将流模型\(u_{t}^{\theta}\)的训练问题简化为以下三步：（i）寻找条件概率路径\(p_{t | Z}(x | z)\)，使其对应的边际概率路径\(p_{t}(x)\)满足式（4.5）中的边界条件；（ii）寻找能生成该条件概率路径的条件速度场\(u_{t}(x | z)\)；（iii）利用条件流匹配（Conditional Flow Matching）损失进行训练（参见式（4.23））。本节将具体探讨如何执行步骤（i）和（ii），即如何设计此类条件概率路径与条件速度场。

现在，我们将提出一种灵活的方法，通过条件流的特定构造来设计此类条件概率路径和速度场。其思路如下：定义一个流模型\(X_{t | 1}\)（类似于式(3.16)），使其满足边界条件(4.6)，然后通过求导（式(3.20)）从\(X_{t | 1}\)中提取速度场。该过程同时定义了\(p_{t | 1}(x | x_{1})\)和\(u_{t}(x | x_{1})\)。更详细地说，定义条件流模型为：
\[X_{t | 1}=\psi_{t}\left(X_{0} | x_{1}\right), \quad \text{其中 } X_{0} \sim \pi_{0 | 1}\left(\cdot | x_{1}\right), \tag{4.28}\]
式中，\(\psi:[0,1) ×\mathbb{R}^{d} ×\mathbb{R}^{d} \to \mathbb{R}^{d}\)是一个条件流，其定义为：
\[
\psi_{t}\left(x | x_{1}\right)=
\begin{cases} 
x & t=0 \\ 
x_{1} & t=1 
\end{cases}, \tag{4.29}
\]

该条件流在\((t, x)\)上光滑，且在\(x\)上是微分同胚。（此处的“光滑”是指\(\psi_{t}(x | x_{1})\)关于\(t\)和\(x\)的所有阶导数均存在且连续，即满足\(C^{\infty}([0,1) ×\mathbb{R}^{d}, \mathbb{R}^{d})\)。）这些条件还可进一步放宽至\(C^{2}([0,1) ×\mathbb{R}^{d}, \mathbb{R}^{d})\)，但会牺牲一定的简洁性。）推前映射公式(3.15)定义了\(X_{t | 1}\)的概率密度定义为：
\[p_{t | 1}\left(x | x_{1}\right):=\left[\psi_{t}\left(\cdot | x_{1}\right)_{\sharp} \pi_{0 | 1}\left(\cdot | x_{1}\right)\right](x), \tag{4.30}\]

尽管在条件流匹配（CFM）损失的实际优化过程中，我们并不需要用到这个表达式，但它在理论上可用于证明\(p_{t | 1}\)满足式(4.6)中的两个边界条件。首先，根据式(4.29)，\(\psi_{0}(\cdot | x_{1})\)是恒等映射，这使得在时刻\(t=0\)时，\(\pi_{0 | 1}(\cdot | x_{1})\)保持不变。其次，\(\psi_{1}(\cdot | x_{1})=x_{1}\)是常值映射，当\(t \to 1\)时，所有概率质量都会集中到\(x_{1}\)处。此外需要注意，对于\(t \in[0,1)\)，在\(t \in[0,1)\)上\(\psi_{t}(\cdot | x_{1})\)是一个光滑的微分同胚。因此，根据流与速度场的等价性（详见3.4.1节），存在唯一的光滑条件速度场（参见式(3.20)），其形式为：
\[u_{t}\left(x | x_{1}\right)=\dot{\psi}_{t}\left(\psi_{t}^{-1}\left(x | x_{1}\right) | x_{1}\right) . \tag{4.31}\]

综上，我们已将“寻找条件路径及对应生成速度”这一任务进一步简化为：只需构建一个满足式(4.29)的条件流\(\psi_{t}(\cdot | x_{1})\)即可。在4.7节中，我们将选取一个特别简单且具备理想性质的\(\psi_{t}(x | x_{1})\)（即条件最优传输流），该条件流将推导出1节中所述的标准流匹配算法；在4.8节中，我们将讨论一类特定且知名的条件流——仿射流，这类流包含扩散模型相关文献中的一些已知示例。在5节中，我们将利用条件流来定义流形上的流匹配，以此展现该方法的灵活性。

### 4.6.1 再论条件流匹配损失  
我们令\(Z=X_{1}\)，并采用条件流的方式定义条件概率路径与速度，以此重新审视条件流匹配（CFM）损失（式(4.23)），可得：
\[
\begin{aligned} 
\mathcal{L}_{CFM}(\theta) & =\mathbb{E}_{t, X_{1}, X_{t} \sim p_{t}\left(\cdot | X_{1}\right)} D\left(u_{t}\left(X_{t} | X_{1}\right), u_{t}^{\theta}\left(X_{t}\right)\right) \\ 
& \stackrel{(3.4)}{=} \mathbb{E}_{t,\left(X_{0}, X_{1}\right) \sim \pi_{0,1}} D\left(\dot{\psi}_{t}\left(X_{0} | X_{1}\right), u_{t}^{\theta}\left(X_{t}\right)\right) 
\end{aligned} 
\tag{4.32}
\]

其中，第二个等式利用了“无意识统计学家定律”（Law of Unconscious Statistician），且满足\(X_{t}=\psi_{t}(X_{0} | X_{1})\)；同时，结合下式：
\[
u_{t}\left(X_{t} | X_{1}\right) \stackrel{(4.31)}{=} \dot{\psi}_{t}\left(\psi_{t}^{-1}\left(\psi_{t}\left(X_{0} | X_{1}\right) | X_{1}\right) | X_{1}\right)=\dot{\psi}_{t}\left(X_{0} | X_{1}\right) . 
\tag{4.33}
\]

根据命题1，损失函数(4.32)的极小值点具有如（Liu等人，2022）所述的形式，即：
\[u_{t}(x)=\mathbb{E}\left[\dot{\psi}_{t}\left(X_{0} | X_{1}\right) | X_{t}=x\right] . \tag{4.34}\]

### 4.6.2 基于条件流构建的概率路径的边际化技巧

接下来，我们将介绍一种适用于“由条件流构建的概率路径”的边际化技巧。为此需注意，若\(\pi_{0 | 1}(\cdot | x_{1})\)属于\(C^1\)函数类（即一阶连续可微），则通过构造可知\(p_{t}(x | x_{1})\)同样属于\(C^1\)函数类；此外，若满足以下条件：
\[
\mathbb{E}_{t,\left(X_{0}, X_{1}\right) \sim \pi_{0,1}}\left\| \dot{\psi}_{t}\left(X_{0} | X_{1}\right)\right\| <\infty 
\tag{4.35}
\]

则\(u_{t}(x | x_{1})\)是条件可积的。

**推论1**：假设目标分布\(q\)具有有界支撑集，对于某些满足\(q(x_{1})>0\)的\(x_{1}\)，条件耦合\(\pi_{0 | 1}(\cdot | x_{1})\)属于\(C^{1}(\mathbb{R}^{d})\)函数类（即一阶连续可微）且严格为正，同时条件流\(\psi_{t}(x | x_{1})\)满足式(4.29)与式(4.35)。则分别由式(4.30)和式(4.31)定义的\(p_{t | 1}(x | x_{1})\)（条件概率路径）与\(u_{t}(x | x_{1})\)（条件速度场），可确定一个边际速度场\(u_{t}(x)\)，该边际速度场能生成用于插值源分布\(p\)与目标分布\(q\)的边际概率路径\(p_{t}(x)\)。

证明：若对于某个\(x_{1}\)，有\(\pi_{0|1}(\cdot | x_1) \ge 0\)，使得\(q(x_{1})>0\)，则可得对所有\(t \in[0,1)\)，\(p_{t | 1}\left(x | x_{1}\right)>0\)，且\(p_{t | 1}\left(x | x_{1}\right)\)是\(C^1\)函数（定义详见式(4.30)与式(3.15)）。此外，由式(4.31)定义的\(u_{t}(x | x_{1})\)是光滑的，且满足：
\[
\begin{aligned} 
\int_{0}^{1} \int\left\| u_{t}\left(x | x_{1}\right)\right\| p_{t | 1}\left(x | x_{1}\right) q\left(x_{1}\right) d x_{1} d x d t & =\mathbb{E}_{t, X_{1} \sim q, X_{t} \sim p_{t | 1}\left(\cdot | X_{1}\right)}\left\| u_{t}\left(X_{t} | X_{1}\right)\right\| \\ 
& \stackrel{(3.4)}{=} \mathbb{E}_{t, X_{1} \sim q, X_{0} \sim \pi_{0 | 1}\left(\cdot | X_{1}\right)}\left\| u_{t}\left(\psi_{t}\left(X_{0} | X_{1}\right) | X_{1}\right)\right\| \\ 
& \stackrel{(4.33)}{=} \mathbb{E}_{t,\left(X_{0}, X_{1}\right) \sim \pi_{0,1}}\left\| \dot{\psi}_{t}\left(X_{0} | X_{1}\right)\right\| \\ 
& <\infty . 
\end{aligned}
\]

因此，\(u_{t}(x | x_{1})\)是条件可积的（参见式(4.13)）。根据定理3，边际速度场\(u_{t}(x)\)生成边际概率路径\(p_{t}(x)\)。由于由式(4.30)定义的\(p_{t | 1}(x | x_{1})\)满足式(4.6)中的边界条件，由此可推出\(p_{t}(x)\)能对源分布\(p\)与目标分布\(q\)进行插值。■

本推论将作为一种工具，用于证明：对条件流的特定选择，会得到能生成边际概率路径\(p_{t}(x)\)的边际速度场\(u_{t}(x)\)。

### 4.6.3 采用其他条件的条件流  

存在多种条件变量\(Z\)的选择，但本质上这些选择都是等价的。主要选择包括：固定目标样本\(Z=X_{1}\)（Lipman等人，2022）、固定源样本\(Z=X_{0}\)（Esser等人，2024），或采用双向条件\(Z=(X_{0}, X_{1})\)（Albergo与Vanden-Eijnden，2022；Liu等人，2022；Pooladian等人，2023；Tong等人，2023）。

我们重点关注双向条件\(Z=(X_0, X_1)\)。按照上述流匹配（FM）框架，我们当前旨在构建一个条件概率路径\(p_{t|0,1}(x|x_0, x_1)\)以及对应的生成速度场\(u_t(x|x_0, x_1)\)，使其满足：
\[p_{0|0,1}(x|x_0, x_1) = \delta_{x_0}(x), \quad \text{且 } p_{1|0,1}(x|x_0, x_1) = \delta_{x_1}(x). \tag{4.36}\]

我们将保持本节讨论的严谨性，因为这部分内容需要用到狄拉克δ函数（δ函数），而截至目前，我们已有的推导仅涉及概率密度（尚未涵盖一般分布）。为构建这样的路径，我们可以考虑一个插值函数（Albergo与Vanden-Eijnden，2022），该插值函数通过\(X_{t|0,1}=\psi_{t}(x_0, x_1)\)定义，其中函数\(\psi:[0,1] ×\mathbb{R}^d ×\mathbb{R}^d \to \mathbb{R}^d\)满足与式(4.29)类似的条件，即：
\[
\psi_{t}\left(x_{0}, x_{1}\right)= 
\begin{cases} 
x_{0} & t=0 \\ 
x_{1} & t=1 
\end{cases}. \tag{4.37}
\]

因此，\(\psi_{t}(\cdot, x_{1})\)会将狄拉克δ分布\(\delta_{x_{0}}(x)\)推前映射至\(\delta_{x_{1}}(x)\)。现在，我们沿用此前的方式，将条件概率路径定义为：
\[p_{t | 0,1}\left(\cdot | x_{0}, x_{1}\right):=\psi_{t}\left(\cdot, x_{1}\right)_{\sharp} \delta_{x_{0}}(\cdot) \tag{4.38}\]
该条件概率路径满足式(4.36)中的边界约束。Albergo与Vanden-Eijnden（2022）提出的随机插值函数定义为：
\[X_{t}=\psi_{t}\left(X_{0}, X_{1}\right) \sim p_{t}(\cdot)=\int p_{t | 0,1}\left(\cdot | x_{0}, x_{1}\right) \pi_{0,1}\left(x_{0}, x_{1}\right) d x_{0} d x_{1} . \tag{4.39}\]

接下来，沿该路径的条件速度也可通过式(3.20)计算得到：
\[u_{t}\left(x | x_{0}, x_{1}\right)=\dot{\psi}_{t}\left(x_{0}, x_{1}\right) \tag{4.40}\]

该条件速度仅在\(x=\psi_{t}(x_{0}, x_{1})\)时有定义。暂且不考虑额外条件，定理3此时大概可推出：生成\(p_{t}(x)\)的边际速度场为
\[
\begin{aligned} 
u_{t}(x) & =\mathbb{E}\left[u_{t}\left(X_{t} | X_{0}, X_{1}\right) | X_{t}=x\right] \\ 
& =\mathbb{E}\left[\dot{\psi}_{t}\left(X_{0}, X_{1}\right) | X_{t}=x\right], 
\end{aligned}
\]

该式与以\(X_1\)为条件的情况（式(4.34)）具有相同的边际公式，但此处的条件流\(\psi_{t}(x_{0}, x_{1})\)约束似乎更宽松——如今仅要求它是一个插值函数，而不再需要满足此前更为严格的微分同胚条件。然而，更细致的分析会发现，要使\(u_{t}(x)\)成为生成\(p_{t}(x)\)的速度场，仍需满足一些额外条件；仅靠（式(4.37)所定义的）简单插值，即便附加额外的光滑性条件，也不足以保证这一点（而定理3对此是有要求的）。为说明这一点，考虑如下插值函数：
\[
\psi_{t}\left(x_{0}, x_{1}\right)=(1-2 t)_{+}^{\tau} x_{0}+(2 t-1)_{+}^{\tau} x_{1}, \quad \text{其中 } (s)_{+}=\text{ReLU}(s), \tau>2,
\]

这是一个在时间上属于\(C^2([0,1])\)的插值函数，对于所有\(x_0\)、\(x_1\)，它会在\(t=0.5\)时刻将所有概率质量集中到位置0处。也就是说，\(\mathbb{P}(X_{\frac{1}{2}}=0)=1\)。因此，若假设\(u_{t}(x)\)确实能生成\(p_{t}(x)\)，则其在\(t=\frac{1}{2}\)处的边际分布为\(\delta_0\)（狄拉克delta分布）；又因为流既具有马尔可夫性（如式(3.17)所示）又具有确定性，所以对于所有\(t>0.5\)，其边际分布都必须是delta函数——这就会产生矛盾，因为\(X_{1}=\psi_{1}(X_{0}, X_{1}) \sim q\)（目标分布），而目标分布\(q\)通常并非delta函数。Albergo与Vanden-Eijnden（2022）以及Liu等人（2022）提出了一些额外条件，可保证\(u_{t}(x)\)确实能生成\(p_{t}(x)\)，但与定理3的条件相比，这些额外条件的验证难度稍大。下文将说明如何通过实际验证定理3的条件，来确认目标概率路径确实能由相应的边际速度场生成。

然而，当\(\psi_{t}(x_{0}, x_{1})\)额外满足“固定\(x_{1}\)时关于\(x_{0}\)是微分同胚、固定\(x_{0}\)时关于\(x_{1}\)是微分同胚”这一条件时，上述三种构造（分别以\(X_1\)、\(X_0\)、\((X_0,X_1)\)为条件）会得到相同的边际速度场（由式(4.34)定义）和相同的边际概率路径\(p_{t}\)（由\(X_{t}=\psi_{t}(X_{0}, X_{1})=\psi_{t}(X_{0} | X_{1})=\psi_{t}(X_{1} | X_{0})\)定义）。

### 4.7 最优传输与线性条件流  

我们现在要问：如何找到一个实用的条件流\(\psi_{t}(x | x_{1})\)？一种方法是将其选为某一自然代价泛函的极小值点，且理想情况下该条件流需具备一些优良性质。这类代价泛函的一个典型例子是带二次代价的动态最优传输问题（Villani等人，2009；Villani，2021；Peyré等人，2019），其形式化表述如下：

\[
\left(p_{t}^{*}, u_{t}^{*}\right)=\underset{p_{t}, u_{t}}{arg min } \int_{0}^{1} \int\left\| u_{t}(x)\right\| ^{2} p_{t}(x) d x d t \quad (\text{动能}) \tag{4.41a}
\]

\[
\text{s.t. } p_{0}=p,\ p_{1}=q \quad (\text{插值约束}) \tag{4.41b}
\]

\[
\left.\frac{d}{d t} p_{t}+\text{div}\left(p_{t} u_{t}\right)=0 \quad (\text{连续性方程}) \tag{4.41c}\right.
\]

上文所述的\((p_{t}^{*}, u_{t}^{*})\)通过式(3.19)定义了一个具有如下形式的流：
\[
\psi_{t}^{*}(x)=t \phi(x)+(1-t) x, \quad(4.42)
\]
该流被称为最优传输（OT）位移插值函数（McCann，1997），其中\(\phi: \mathbb{R}^{d} \to \mathbb{R}^{d}\)表示最优传输映射。此外，通过定义随机变量
\[
\text{当 } X_{0} \sim p \text{ 时，} X_{t}=\psi_{t}^{*}\left(X_{0}\right) \sim p_{t}^{*} \tag{4.43}
\]
最优传输位移插值函数也可用于求解流匹配问题（式(4.1)）。

最优传输框架会生成平直的样本轨迹，其形式为：
\[
X_{t}=\psi_{t}^{*}\left(X_{0}\right)=X_{0}+t\left(\phi\left(X_{0}\right)-X_{0}\right),
\]
该轨迹具有恒定速度\(\phi(X_{0})-X_{0}\)。总体而言，此类轨迹更易于通过常微分方程（ODE）求解器进行采样——具体来说，对于目标样本\(X_{1}\)，此处仅需执行一步欧拉法（式(3.21)）即可精确求解得到。

现在，我们可尝试将边际速度公式（式(4.34)）代入最优传输问题（式(4.41)）中，进而寻找最优的\(\psi_{t}(x | x_{1})\)。尽管这一过程看似具有挑战性，但我们可转而求解动能的下界——该下界的极小值点很容易确定（Liu等人，2022），具体推导如下：

\[
\int_{0}^{1} \mathbb{E}_{X_{t} \sim p_{t}}\left\| u_{t}\left(X_{t}\right)\right\| ^{2} d t=\int_{0}^{1} \mathbb{E}_{X_{t} \sim p_{t}}\left\| \mathbb{E}\left[\dot{\psi}_{t}\left(X_{0} | X_{1}\right) | X_{t}\right]\right\| ^{2} d t \tag{4.44}
\]

\[
\stackrel{(i)}{\leq} \int_{0}^{1} \mathbb{E}_{X_{t} \sim p_{t}} \mathbb{E}\left[\left\| \dot{\psi}_{t}\left(X_{0} | X_{1}\right)\right\| ^{2} | X_{t}\right] d t
\tag{4.45}
\]

\[
\stackrel{(ii)}{=} \mathbb{E}_{\left(X_{0}, X_{1}\right) \sim \pi_{0,1}} \int_{0}^{1}\left\| \dot{\psi}_{t}\left(X_{0} | X_{1}\right)\right\| ^{2} d t,
\tag{4.46}
\]

其中，步骤(i)使用了詹森不等式（Jensen’s inequality）；步骤(ii)则利用了条件期望的塔性质（参见式(3.11)），并交换了对\(t\)的积分与期望的运算顺序。此时，式(4.46)中的被积函数可对每个\((X_{0}, X_{1})\)单独求极小值——这引出关于\(\gamma_{t}=\psi_{t}(x | x_{1})\)的变分问题，具体如下：

\[
\min _{\gamma:[0,1] \to \mathbb{R}^{d}} \int_{0}^{1}\left\| \dot{\gamma}_{t}\right\| ^{2} d t \tag{4.47a}
\]

\[
\text{s.t. } \gamma_{0}=x, \gamma_{1}=x_{1} \tag{4.47b}
\]

该问题可通过欧拉-拉格朗日方程（Euler-Lagrange equations）求解（Gelfand等人，2000），在本题中，该方程的形式为\(\frac{d^2}{dt^2}\gamma_t = 0\)。结合边界条件后，我们可得到极小值点（即最优条件流）：
\[
\psi_t(x|x_1) = t x_1 + (1 - t)x \tag{4.48}
\]

需注意，尽管未对\(\psi_t(x|x_1)\)施加相关约束，但该条件流的选择仍满足：当\(t \in [0,1)\)时，其关于\(x\)是微分同胚（diffeomorphism），且关于\(t\)和\(x\)均光滑——这恰好符合条件流的要求。

可以得到以下结论：
1. 在所有条件流中，线性条件流中使动能下界达到最小。
2. 若目标分布\(q\)仅包含单个数据点（即\(q(x)=\delta_{x_{1}}(\cdot)\)），则式(4.48)中的线性条件流即为最优传输映射（Lipman等人，2022）。实际上，在此情况下，\(X_{t}=\psi_{t}(X_{0} | x_{1}) \sim p_{t}\)，且\(X_{0}=\psi_{t}^{-1}(X_{t} | x_{1})\)是关于\(X_{t}\)的函数——这使得\(\mathbb{E}[\dot{\psi}_{t}(X_{0} | x_{1}) | X_{t}]=\dot{\psi}_{t}(X_{0} | x_{1})\)，因此前文推导中的步骤(ii)将变为等式。
**定理5**：若\(q=\delta_{x_{1}}\)（即目标分布为狄拉克δ分布），则动态最优传输（OT）问题（式(4.41)）存在解析解，该解析解由式(4.48)中的最优传输（OT）位移插值函数给出。

3. 将线性条件流代入式(4.46)，可得：
\[
\int_{0}^{1} \mathbb{E}_{X_{t} \sim p_{t}}\left\| u_{t}\left(X_{t}\right)\right\| ^{2} d t \leq \mathbb{E}_{\left(X_{0}, X_{1}\right) \sim \pi_{0,1}} \int_{0}^{1}\left\| X_{1}-X_{0}\right\| ^{2} d t
\]
该式表明，边际速度场\(u_{t}(x)\)的动能不大于原始耦合分布\(\pi_{0,1}\)对应的动能（Liu等人，2022）。

式(4.48)中的条件流尤其属于仿射流，因此这启发我们对仿射条件流族展开研究，相关内容将在下文讨论。

### 4.8 仿射条件流  
在上一节中，我们发现线性（条件最优传输，Conditional-OT）流是所有条件流中能使动能下界达到最小的流。该线性条件流是更广泛的仿射条件流族中的一个特例，本节将对仿射条件流族展开探讨。仿射条件流的形式为：
\[
\psi_{t}\left(x | x_{1}\right)=\alpha_{t} x_{1}+\sigma_{t} x \tag{4.50}
\]
其中，\(\alpha_t, \sigma_{t}:[0,1] \to [0,1]\) 是光滑函数，且满足以下条件：
\[
\alpha_{0}=0=\sigma_{1},\ \alpha_{1}=1=\sigma_{0},\ \text{且对 } t \in(0,1) \text{ 有 } \dot{\alpha}_{t},-\dot{\sigma}_{t}>0 \tag{4.51}
\]

我们将参数对\((\alpha_{t}, \sigma_{t})\)称为调度器（scheduler）。上述导数条件确保\(\alpha_{t}\)是严格单调递增函数，而\(\sigma_{t}\)是严格单调递减函数。对于每个\(t \in[0,1)\)，条件流（式(4.50)）是关于\(x\)的简单仿射映射，且满足条件（式(4.29)）。与之关联的边际速度场（式(4.34)）为：
\[
u_{t}(x)=\mathbb{E}\left[\dot{\alpha}_{t} X_{1}+\dot{\sigma}_{t} X_{0} | X_{t}=x\right] \tag{4.52}
\]

借助推论1（corollary 1）可证明：若采用独立耦合（independent coupling），且源分布密度\(p\)光滑、严格为正且具有有限二阶矩（例如，高斯分布\(p=\mathcal{N}(\cdot | 0, I)\)），则速度场\(u_{t}\)会生成一条插值于分布\(p\)与\(q\)之间的概率路径\(p_{t}\)。我们将这一结果（其对“流匹配（Flow Matching）”的应用具有重要意义）正式表述为如下定理。

**定理6**
假设目标分布\(q\)的支撑集有界，源分布\(p\)属于\(C^{1}(\mathbb{R}^{d})\)（即\(p\)是定义在\(\mathbb{R}^{d}\)上的一阶连续可微函数），且其密度函数严格为正、二阶矩有限；源分布\(p\)与目标分布\(q\)通过独立耦合\(\pi_{0,1}(x_{0}, x_{1})=p(x_{0})q(x_{1})\)相关联。设\(p_{t}(x)=\int p_{t|1}(x | x_{1})q(x_{1})dx_{1}\)由式(4.30)定义，其中\(\psi_{t}\)由式(4.50)定义。则边际速度场（式(4.52)）会生成插值于分布\(p\)与\(q\)之间的概率路径\(p_{t}\)。

证明：我们应用推论1（corollary 1）进行证明。首先，注意到由假设可知，条件分布\(\pi_{0|1}(\cdot | x_1) = p(\cdot)\)属于\(C^1\)（一阶连续可微函数类），且在全体定义域上密度为正。其次，由式(4.50)定义的\(\psi_t\)满足式(4.29)的条件。最后，我们只需验证式(4.35)的成立，具体推导如下：
\[
\begin{aligned} 
\mathbb{E}_{t,(X_{0}, X_{1})}\left\| \dot{\psi}(X_{0} | X_{1})\right\| & =\mathbb{E}_{t,(X_{0}, X_{1})}\left\| \dot{\alpha}_{t} X_{1}+\dot{\sigma}_{t} X_{0}\right\| \\ 
& \leq \mathbb{E}_{t}\left|\dot{\alpha}_{t}\right| \mathbb{E}_{X_{1}}\left\| X_{1}\right\| +\mathbb{E}_{t}\left|\dot{\sigma}_{t}\right| \mathbb{E}_{X_{0}}\left\| X_{0}\right\| \\ 
& =\mathbb{E}_{X_{1}}\left\| X_{1}\right\| +\mathbb{E}_{X_{0}}\left\| X_{0}\right\| \\ 
& <\infty, 
\end{aligned}
\]

其中，最后一个不等式成立的依据是：服从分布\(q\)的\(X_1\)具有有界支撑集，而服从分布\(p\)的\(X_0\)具有有界二阶矩。■

在这种仿射情形下，CFM损失（式(4.32)）具有如下形式：
\[
\mathcal{L}_{\text{CFM}}(\theta) = \mathbb{E}_{t,(X_0,X_1) \sim \pi_{0,1}} D\left(\dot{\alpha}_t X_1 + \dot{\sigma}_t X_0, u_t^\theta(X_t)\right) \tag{4.53}
\]

#### 4.8.1 速度参数化
在仿射情形下，边际速度场\(u_t\)具有多种参数化形式，每种形式都可利用4.5节介绍的流匹配（Flow Matching）损失进行学习。为推导这些参数化形式，需将仿射路径的等价表达式  
\[X_{t}=\alpha_{t} X_{1}+\sigma_{t} X_{0} \Leftrightarrow X_{1}=\frac{X_{t}-\sigma_{t} X_{0}}{\alpha_{t}} \Leftrightarrow X_{0}=\frac{X_{t}-\alpha_{t} X_{1}}{\sigma_{t}} \tag{4.54}\]  
代入边际速度公式（式(4.52)），可得：  

\[u_{t}(x) =\dot{\alpha}_{t} \mathbb{E}\left[X_{1} | X_{t}=x\right]+\dot{\sigma}_{t} \mathbb{E}\left[X_{0} | X_{t}=x\right]  \tag{4.55}\]
\[u_{t}(x) =\frac{\dot{\sigma}_{t}}{\sigma_{t}} x+\left[\dot{\alpha}_{t}-\alpha_{t} \frac{\dot{\sigma}_{t}}{\sigma_{t}}\right] \mathbb{E}\left[X_{1} | X_{t}=x\right] \tag{4.56}\]
\[u_{t}(x) =\frac{\dot{\alpha}_{t}}{\alpha_{t}} x+\left[\dot{\sigma}_{t}-\sigma_{t} \frac{\dot{\alpha}_{t}}{\alpha_{t}}\right] \mathbb{E}\left[X_{0} | X_{t}=x\right] \tag{4.57}\]

其中用到了条件期望的性质\(\mathbb{E}[Z | Z=z]=z\)。接下来，定义确定性函数：  
\[x_{1 | t}(x)=\mathbb{E}\left[X_{1} | X_{t}=x\right]，称为x₁预测（目标端预测） \tag{4.58}\]  

\[x_{0 | t}(x)=\mathbb{E}\left[X_{0} | X_{t}=x\right]，称为x₀预测（源端预测） \tag{4.59}\]

这为边际速度场\(u_t\)提供了另外两种参数化方式：一种通过\(x_1\)预测（即\(x_{1|t}\)，式(4.56)），另一种通过\(x_0\)预测（即\(x_{0|t}\)，式(4.57)）。表1给出了这些参数化方式之间的转换公式。这些参数化方式也可利用条件匹配（Conditional Matching）损失进行学习，该损失与式(4.23)类似。具体而言，对于任意函数  
\[g_t(x) := \mathbb{E}\left[f_t(X_0, X_1) \mid X_t = x\right] \tag{4.60}\] 

其中\(f_t(X_0, X_1)\)是一个随机变量（RV），定义为\(X_0\)与\(X_1\)的时变函数，可通过最小化如下形式的匹配损失来学习：  
\[
\mathcal{L}_M(\theta) = \mathbb{E}_{t, X_t \sim p_t} D\left(g_t(X_t), g_t^\theta(X_t)\right) \tag{4.61}
\]

该损失与条件匹配（Conditional Matching，CM）损失具有相同的梯度，条件匹配损失的表达式为：
\[
\mathcal{L}_{\text{CM}}(\theta) = \mathbb{E}_{t,(X_0,X_1) \sim \pi_{0,1}} D\left(f_t(X_0,X_1), g_t^\theta(X_t)\right) \tag{4.62}
\]

**定理7**对于\(X_0\)与\(X_1\)的任意函数\(f_t(X_0, X_1)\)，匹配损失（Matching loss）与条件匹配损失（Conditional Matching loss）的梯度完全一致，即：
\[
\nabla_{\theta} \mathcal{L}_M(\theta) = \nabla_{\theta} \mathcal{L}_{CM}(\theta) \tag{4.63}
\]
具体而言，条件匹配损失的极小值点为如下条件期望：
\[
g_t^\theta(x) = \mathbb{E}\left[f_t(X_0, X_1) \mid X_t = x\right] \tag{4.64}
\]

**速度参数化中的奇点问题**:
从表面上看，式(4.56)的系数会在\(t \to 1\)时趋于无穷大，类似地，式(4.57)的系数会在\(t \to 0\)时趋于无穷大。若条件期望\(\mathbb{E}[X_1 \mid X_0 = x]\)与\(\mathbb{E}[X_0 \mid X_1 = x]\)存在（当源分布密度\(p(x) > 0\)且目标分布密度\(q(x) > 0\)时，该条件成立），则理论上这些并非本质奇点——这意味着\(x_{1|t}\)（\(x_1\)预测）与\(x_{0|t}\)（\(x_0\)预测）中的奇点，会与参数化系数的奇点相互抵消。

然而在实际情况中，当可学习的\(x_{1|t}^\theta\)（参数化的\(x_1\)预测）与\(x_{0|t}^\theta\)（参数化的\(x_0\)预测）在构造上具有连续性，因而无法完美回归其目标\(x_{1|t}\)与\(x_{0|t}\)时，这些奇点仍可能引发问题。

为理解如何解决这些潜在问题，回顾式(4.55)，并考虑当\(t \to 0\)时，速度场的极限形式为\(u_0(x) = \dot{\alpha}_0 \mathbb{E}[X_1 \mid X_0 = x] + \dot{\sigma}_0 x\)；当\(t \to 1\)时，速度场的极限形式为\(u_1(x) = \dot{\alpha}_1 x + \dot{\sigma}_1 \mathbb{E}[X_0 \mid X_1 = x]\)。在许多受关注的场景中，均可计算出上述极限形式。

回到我们的示例，独立耦合\(\pi_{0,1}(x_0, x_1) = \mathcal{N}(x_0 \mid 0, I) q(x_1)\)，并假设\(\mathbb{E}_{X_1}X_1 = 0\)，可推得\(u_0(x) = \dot{\sigma}_0 x\)且\(u_1(x) = \dot{\alpha}_1 x\)。上述表达式可分别用于解决以下场景中的奇点问题：当\(t \to 1\)时，由\(x_{1|t}\)转换为\(u_t(x)\)；当\(t \to 0\)时，由\(x_{0|t}\)转换为\(u_t(x)\)。

#### 4.8.2 训练后速度调度器调整

仿射条件流允许从一个边缘速度场\(u_{t}(x)\)（基于调度器\((\alpha_{t}, \sigma_{t})\)和任意数据耦合\(\pi 0,1\)）到另一个边缘速度场\(\bar{u}_{r}(x)\)（基于不同的调度器\((\bar{\alpha}_{r}, \bar{\sigma}_{r})\)和相同的数据耦合\(\pi_{0,1}\)）的闭式变换。这种变换有助于将训练好的速度场适配到不同的调度器，可能会提高采样效率和生成质量（Karras等人，2022；Shaul等人，2023b；Pokle等人，2023）。接下来，定义两个条件流之间的尺度-时间（ST）变换\((s_{r}, t_{r})\)：

\[\overline{\psi}_{r}\left(x_{0} | x_{1}\right)=s_{r} \psi_{t_{r}}\left(x_{0} | x_{1}\right), \tag{4.65}\]

其中\(\psi_{t}(x_{0} | x_{1})=\alpha_{t} x_{1}+\sigma_{t} x_{0}\)、\(\bar{\psi}_{r}(x_{0} | x_{1})=\bar{\alpha}_{r} x_{1}+\bar{\sigma}_{r} x_{0}\)和s\(s, t:[0,1] \to \mathbb{R}_{≥0}\)是时间-尺度重新参数化。求解（4.65）可得

\[\begin{aligned} t_{r} & =\rho^{-1}(\overline{\rho}(r)) \\ s_{r} & =\overline{\sigma}_{r} / \sigma_{t_{r}}, \tag{4.66}\end{aligned} \]

其中我们通过以下方式定义信噪比

\[\begin{aligned} \rho(t) & =\frac{\alpha_{t}}{\sigma_{t}} \\ \overline{\rho}(t) & =\frac{\overline{\alpha}_{t}}{\overline{\sigma}_{t}}, \tag{4.67}\end{aligned} \]

假设它是一个可逆函数。新调度器\((\bar{\alpha}_{r}, \bar{\sigma}_{r})\)对应的边缘速度\(\bar{u}_{r}(x)\)遵循以下表达式

\[\begin{aligned} \overline{u}_{r}(x) & =\mathbb{E}\left[\dot{X}_{r} | \overline{X}_{r}=x\right] \\ & \stackrel{(4.65)}{=} \mathbb{E}\left[\dot{s}_{r} X_{t_{r}}+s_{r} \dot{X}_{t_{r}} \dot{t}_{r} | s_{r} X_{t_{r}}=x\right] \\ & =\dot{s}_{r} \mathbb{E}\left[X_{t_{r}} | X_{t_{r}}=\frac{x}{s_{r}}\right]+s_{r} \dot{t}_{r} \mathbb{E}\left[\dot{X}_{t_{r}} | X_{t_{r}}=\frac{x}{s_{r}}\right] \\ & =\frac{\dot{s}_{r}}{s_{r}} x+s_{r} \dot{t}_{r} u_{t_{r}}\left(\frac{x}{s_{r}}\right), \end{aligned} \]

其中如前所述\(\bar{X}_{r}=\bar{\psi}_{r}(X_{0} | X_{1})\)和\(X_{t}=\psi_{t}(X_{0} | X_{1})\)。最后这一项可用于在训练后更换调度器。

调度器的等效性。上述公式的另一个重要推论是，所有调度器在理论上都会在时间\(t=1\)产生相同的采样结果（Shaul等人，2023a）。也就是说，

\[\overline{\psi}_{1}\left(x_{0}\right)=\psi_{1}\left(x_{0}\right), for all x_{0} \in \mathbb{R}^{d}. \tag{4.68}\]

为了说明这一点，记\(\bar{\psi}_{r}(x)\)为由\(\bar{u}_{t}(x)\)定义的流，对\(\tilde{\psi}_{r}(x):=s_{r} \psi_{i_{r}}(x)\)关于r求导，并注意到它也满足
\[\frac{d}{d t} \tilde{\psi}_{r}(x)=\overline{u}_{r}\left(\tilde{\psi}_{r}(x)\right) . \tag{4.69}\]

因此，根据常微分方程解的唯一性，我们有\(\bar{\psi}_{r}(x)=\tilde{\psi}_{r}(x)=s_{r} \psi_{t_{r}}(x)\)。现在，为了避免处理无限信噪比，假设调度器满足\(\sigma_{1}=\epsilon=\bar{\sigma}_{1}\)（对于任意\(\epsilon>0\)，除了式（4.51）之外），那么对于\(r=1\)，我们有\(t_{1}=1\)和\(s_{1}=1\)，因此式（4.68）成立。

#### 4.8.3 高斯路径
在撰写本文时，最受欢迎的仿射概率路径类别由独立耦合\(\pi_{0,1}(x_{0}, x_{1})=p(x_{0}) q(x_{1})\)和高斯源分布\(p(x)=N(x | 0, \sigma^{2} I)\)实例化。由于高斯分布在仿射变换下具有不变性，由此产生的条件概率路径形式为
\[p_{t | 1}\left(x | x_{1}\right)=\mathcal{N}\left(x | \alpha_{t} x_{1}, \sigma_{t}^{2} I\right) . \tag{4.70}\]

此案例包含了由标准扩散模型生成的概率路径（尽管在扩散过程中，生成是随机的且遵循随机微分方程，但其具有相同的边际概率）。两个例子是保方差（VP）路径和爆方差（VE）路径（Song等人，2021），它们通过选择以下调度器来定义：

\[\alpha_{t} \equiv 1, \sigma_{0} \gg 1, \sigma_{1}=0 ; \quad(VP)\]

\[\alpha_{t}=e^{-\frac{1}{2} \beta_{t}}, \sigma_{t}=\sqrt{1-e^{-\beta_{t}}}, \beta_{0} \gg 1, \beta_{1}=0 . \quad(VE)\]

在前面的等式中，“\(" \gg 1 "\)”需要一个足够大的标量，使得\(p_{0}(x)=\int p_{0 | 1}(x | x_{1}) q(x_{1}) d x_{1}\)接近\(t=0\)的已知高斯分布，也就是说，VE对应的高斯分布是\(N(\cdot | 0, \sigma_{0}^{2} I)\)，VP对应的是\(N(\cdot | 0, I)\)。请注意，在这两种情况下，与（4.51）中的FM路径不同，\(p_{t}(x)\)在\(t=0\)处并不能精确重现p。

在高斯情况下，有一个具有简单形式的有用量是得分，其定义为对数概率的梯度。具体而言，式（4.70）中条件路径的得分遵循以下表达式：

\[\nabla log p_{t | 1}\left(x | x_{1}\right)=-\frac{1}{\sigma_{t}^{2}}\left(x-\alpha_{t} x_{1}\right) . \tag{4.71}\]

对应于边缘概率路径（4.4）的得分是
\[\nabla log p_{t}(x)=\int \frac{\nabla p_{t | 1}\left(x | x_{1}\right) q\left(x_{1}\right)}{p_{t}(x)} d x_{1} \tag{4.72}\]

\[=\int \nabla log p_{t | 1}\left(x | x_{1}\right) \frac{ p_{t | 1}\left(x | x_{1}\right) q\left(x_{1}\right)}{p_{t}(x)} d x_{1} \tag{4.73}\]

\[=\mathbb{E} \left[ \nabla log p_{t | 1}\left(X_t | X_{1}\right) | X_t = x \right] \tag{4.74}\]

\[\stackrel{(4.71)}{=} \mathbb{E}\left[-\frac{1}{\sigma_{t}^{2}}\left(X_{t}-\alpha_{t} X_{1}\right) | X_{t}=x\right] \tag{4.75}\]

\[\stackrel{(4.54)}{=} \mathbb{E}\left[-\frac{1}{\sigma_{t}} X_{0} | X_{t}=x\right] \tag{4.76}\]

\[\stackrel{(4.59)}{=}-\frac{1}{\sigma_{t}} x_{0 | t}(x), \tag{4.77}\]

where we borrow the notation \(x_{0 | t}\) from (4.59). The literature on diffusion refers to \(x_{0}\) -prediction ( \((x_{0 | t})\) as

其中我们借鉴了（4.59）中的符号\(x_{0 | t}\)。关于扩散的文献将\(x_{0}\) -预测（\((x_{0 | t})\)称为噪声预测，或ϵ预测。上面的公式表明，该分数与\(x_{0}\)预测成正比，并提供了一种转换规则——对于高斯路径情况——从分数到其他参数化形式，如表1所示。

边际速度的动力学最优性。根据上述推导的转换公式（表1），高斯路径的边际速度可以写成以下形式：

\[\begin{aligned} u_{t}(x) & =\frac{\dot{\alpha}_{t}}{\alpha_{t}} x-\frac{\dot{\sigma}_{t} \sigma_{t} \alpha_{t}-\dot{\alpha}_{t} \sigma_{t}^{2}}{\alpha_{t}} \nabla log p_{t}(x) \quad(4.78)\\ & =\nabla\left[\frac{\dot{\alpha}_{t}}{2 \alpha_{t}}\| x\| ^{2}-\frac{\dot{\sigma}_{t} \sigma_{t} \alpha_{t}-\dot{\alpha}_{t} \sigma_{t}^{2}}{\alpha_{t}} log p_{t}(x)\right] \quad(4.79)\end{aligned}\]

这表明\(u_{t}(x)\)是一个梯度，因此对于由\(p_{t | 1}(x | x_{1})=N(x | \alpha_{t} x_{1}, \sigma_{t}^{2} I)\)定义的固定边际化高斯概率路径\(p_{t}(x)\)而言，其具有动力学最优性（参见例如Villani（2021）第8.1.2节，或Neklyudov等人（2023）定理2.1）。

## 4.9 数据耦合
在开发流匹配训练算法时，我们假设可以抽取样本\((X_{0}, X_{1}) ~\pi_{0,1}(X_{0}, X_{1})\) 来自源p分布和目标q分布的某种耦合\(\pi_{0,1}(x_{0}, x_{1})\)。例如，独立样本\(\pi_{0,1}(x_{0}, x_{1})=p(x_{0}) q(x_{1})\)，即保持边缘分布p和q的最简单耦合，或者作为数据集一部分提供的配对样本\((X_{0}, X_{1}) ~ \pi_{0,1}\)。本节探讨了可用于训练流匹配模型的两个具体耦合示例。

### 4.9.1 成对数据
在成对数据的学习任务中，依赖耦合会自然产生。例如，考虑图像补全任务，其中q是自然图像的分布，p是那些相同图像被遮挡了一个方形区域后的分布。我们的目标不是将噪声转换为数据，而是学习从被遮挡的图像\(x_{0}\)到其补全后的对应图像\(x_{1}\)的映射。由于这是一个定义不明确的问题——许多补全后的图像\(x_{1}\)都与每个被遮挡的图像\(x_{0}\)兼容——解决这个任务可以归结为学习从未知的、依赖数据的耦合\(\pi_{1 | 0}(x_{1} | x_{0})\)中进行采样。

基于这些见解，Liu等人（2023年）、Albergo等人（2024年）提出利用数据依赖耦合来学习桥接模型或流模型，这一简单修改开启了新的应用领域。虽然该对象\(\pi_{1 | 0}(x_{1} | x_{0})\)无法进行采样，通常情况下，人们可以从反向依赖项\(\pi_{0 | 1}(x_{0} | x_{1})\)进行采样。回到图像修复的例子，很容易掩盖已填充的图像\(X_{1} ~ q\)（目标样本）以生成源样本\(X_{0} ~ p\)。为此，指定

\[\pi_{0,1}\left(x_{0}, x_{1}\right)=\pi_{0 | 1}\left(x_{0} | x_{1}\right) q\left(x_{1}\right) . \tag{4.80}\]

因此，我们可以通过以下方式获得一对\((X_{0}, X_{1})\)：（i）绘制\(X_{1} ~ q\)，以及（ii）应用预定义的随机变换，从\(X_{1}\)得到\(x_{0}\)。为了满足推论1的条件（确保源是一个密度）并促进多样性，我们在从\(\pi_{0 | 1}(x_{0} | x_{1})\)中采样时添加噪声。（Liu等人，2023；Albergo等人，2024）在各种应用中展示了这种方法的能力，例如图像超分辨率、图像修复和去模糊，其性能优于基于引导扩散的方法（Saharia等人，2022）。

### 4.9.2 多样本耦合
正如4.7节所讨论的，直线概率路径能使常微分方程模拟的误差更小。因此，自然会提出这样一个问题：我们如何调整训练算法，才能让学习到的速度场产生更直的轨迹？

如上所述，直线轨迹与最优传输（OT）问题相关。具体而言，考虑凸成本泛函\(c: \mathbb{R}^{d} \to \mathbb{R}_{≥0}\)和条件OT流\(\psi_{t}(x_{0} | x_{1})=t x_{1}+(1-t) x_{0}\)。那么，耦合的传输成本对边际传输成本有一个上界（Liu等人，2022；Pooladian等人，2023），即：

\[\mathbb{E}\left[c\left(\psi_{1}\left(X_{0}\right)-X_{0}\right)\right] \leq \mathbb{E}\left[c\left(X_{1}-X_{0}\right)\right], \tag{4.81}\]

其中，将满足\(u_{t}(X_{t})=u_{t}(\psi_{t}(x))=\phi(x)-x\)（其中ϕ是OT映射）的OT解代入左侧后，可从（4.49）中的界理解\(c(x)=\|x\|^{2}\)的情况。因此，通过降低耦合成本，可以构建低成本的边际传输映射。为此，Pooladian等人（2023）提出了多样本耦合，这是一种隐式构建非平凡联合分布\(\pi_{0,1}(x_{0}, x_{1})\)的过程，它引入了源分布和目标分布之间的依赖关系：

1. 样本\(X_{0}^{(i)} \sim p\)与\(X_{1}^{(i)} \sim q\), \(i \in[k]\)相互独立。
2. 由\(\pi^{k}:=\arg min _{\pi \in B_{k}} \mathbb{E}_{\pi}[c(X_{0}^{(i)}-X_{1}^{(j)})]\)构建\(\pi^{k} \in B_{k}\)
3. 从\((X_{0}^{(i)}, X_{1}^{(j)})\)中随机均匀抽样一对\((X_{0}^{(i)}, X_{0}^{(j)})\)，其中满足\(\pi^{k}(i, j)=1\)。

其中\(B_{k}\)是\(k ×k\)双随机矩阵的多面体。

上述过程通过采样隐式地定义了一个联合分布$\pi^k_{0,1}(x_0, x_1)$。这种隐式联合分布保留了边缘分布，并且满足最优性约束（步骤2）（Pooladian等人，2023）。当$k = 1$时，该方法退化为独立耦合。当$k > 1$时，Pooladian等人（2023）证明，与独立耦合相比，该方法的传输成本更低，即$\mathbb{E}_{(x_0,x_1) \sim \pi^k_{0,1}(x_0,x_1)} \left[ c(x_1 - x_0) \right] \leq \mathbb{E}_{X_0 \sim p, X_1 \sim q} \left[ c(X_1 - X_0) \right]$。此外，对于二次成本函数，当$k \to \infty$时，多样本耦合趋近于最优传输（Optimal Transport）成本，并产生直线轨迹（Pooladian等人，2023；Tong等人，2023）。

## 4.10 条件生成与引导
我们现在考虑在引导信号下训练生成模型，以进一步控制生成的样本。这种技术已被证明在众多实际应用中具有价值，例如图像到图像的转换（Saharia等人，2022年）和文本到图像的生成（Nichol等人，2022年；Esser等人，2024年）。在本小节中，我们假设可以获取带标签的目标样本\((x_{1}, y)\)，其中\(y \in Y \subseteq \mathbb{R}^{k}\)是一个标签或引导变量。

### 4.10.1 条件模型
在引导下训练生成模型的一种自然方式是学习从条件分布\(q(x_{1} | y)\)中进行采样，扩散模型和FM模型均展示了这一点（Zheng等人，2023）。遵循图2中的FM蓝图，考虑来自条件目标分布\(q(x_{1} | y)\)的样本，并设定一个简单的（通常是但不一定是高斯的）源分布p。接下来，将引导概率路径设计为条件概率路径的聚合：

\[p_{t | Y}(x | y)=\int p_{t | 1}\left(x | x_{1}\right) q\left(x_{1} | y\right) d x_{1} . \tag{4.82}\]

其中我们假设\(p_{t, 1 | Y}(x, x_{1} | y)=p_{t | 1}(x | x_{1}) q(x_{1} | y)\)，即条件路径不依赖于Y。由此产生的引导概率路径以引导变量\(Y ~ p_{Y}\)为条件，并满足边际端点

\[p_{0 | Y}(\cdot | y)=p(\cdot), p_{1 | Y}(\cdot | y)=q(\cdot | y) . \tag{4.83}\]

引导速度场的形式为

\[u_{t}(x | y)=\int u_{t}\left(x | x_{1}\right) p_{1 | t, Y}\left(x_{1} | x, y\right) d x_{1}, \tag{4.84}\]

其中，根据贝叶斯法则，可得

\[p_{1 | t, Y}\left(x_{1} | x, y\right)=\frac{p_{t | 1}\left(x | x_{1}\right) q\left(x_{1} | y\right)}{p_{t | Y}(x | y)} . \tag{4.85}\]

为了证明\(u_{t}(x | y)\)生成\(p_{t | Y}(x | y)\)，将（4.82）和（4.84）代入（4.14），并注意到在引导情况下，FM/CFM损失保持不变，且能采用定理4证明中出现的相同步骤。在实践中，我们训练一个单一的神经网络\(u_{t}^{\theta}: \mathbb{R}^{d} ×\mathbb{R}^{k} \to \mathbb{R}^{d}\)来为所有y值建模引导边际速度场。然后，CFM损失的引导版本（4.32）遵循表达式
\[\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t,\left(X_{0}, X_{1}, Y\right) \sim \pi_{0,1, Y}} D\left(\dot{\psi}_{t}\left(X_{0} | X_{1}\right), u_{t}^{\theta}\left(X_{t} | Y\right)\right) . \tag{4.86}\]
。

实际上，扩散模型相关文献表明，在大量目标样本\(X_{1}\)共享相同引导信号Y的应用中，引导效果最为显著，例如类别引导（Nichol和Dhariwal，2021）。然而，在引导变量Y具有非重复性且复杂的场景中，比如图像字幕，引导就更具挑战性。

### 4.10.2 分类器引导和无分类器引导
对于使用高斯路径训练的流，可以利用表1（Zheng等人，2023）中所示的条件分布的速度场和得分函数之间的转换，应用分类器引导（Song等人，2021；Dhariwal和Nichol，2021）和无分类器引导（Ho和Salimans，2021）：

\[u_{t}(x | y)=a_{t} x+b_{t} \nabla log p_{t | Y}(x | y) . \tag{4.87}\]

在引导概率路径上应用贝叶斯法则可得出

\[p_{t | Y}(x | y)=\frac{p_{Y | t}(y | x) p_{t}(x)}{p_{Y}(y)} . \tag{4.88}\]

对x、\(\nabla=\nabla_{x}\)取对数和梯度，我们得到了概率路径\(p_{t}(x)\)的得分与其引导对应项\(p_{t | Y}(x | y)\)之间的基本关系：

\[\overbrace{\nabla log p_{t | Y}(x | y)}^{conditional score }=\nabla \overbrace{log p_{Y | t}(y | x)}^{classifier }+\overbrace{\nabla log p_{t}(x)}^{unconditional score } . \tag{4.89}\]

即，两者通过分类器模型\(p_{Y | t}(y | x)\)的得分相关联，该模型尝试根据样本x预测引导变量y。

基于这种关系，Song等人（2021）提出了分类器引导方法，即通过一个时间相关的分类器（在给定\(x ~ p_{t}(x))\)的情况下预测引导变量y）来引导一个无条件模型（参数化为\(\nabla log p_{t}(x))\)），从而从条件模型\(q(x_{1} | y)\)中进行采样。相应的速度场转换为：

\[\tilde{u}_{t}^{\theta, \phi}(x | y)=a_{t} x+b_{t}\left(\nabla log p_{Y | t}^{\phi}(y | x)+\nabla log p_{t}^{\theta}(x)\right)=u_{t}^{\theta}(x)+b_{t} \nabla log p_{Y | t}^{\phi}(y | x), \tag{4.90}\]

其中，\(u_{t}^{\theta}(x)\)是在无条件目标\(q(x)\)上训练的速度场，\(log p_{Y | t}^{\phi}(y | x)\)是具有参数\(\phi \in \mathbb{R}^{m}\)的时间相关分类器。Dhariwal和Nichol（2021）表明，这种方法在类别条件和文本条件下都优于4.10.1节中的条件模型（Nichol等人，2022）。在实践中，由于分类器和无条件得分是分开学习的，通常需要将分类器引导校准为

\[\overline{u}_{t}^{\theta, \phi}(x | y)=u_{t}^{\theta}(x)+b_{t} w \nabla log p_{Y | t}^{\phi}(y | x), \tag{4.91}\]

其中，\(w \in \mathbb{R}\)是分类器尺度，通常选择为\(w>1\)（Dhariwal和Nichol，2021）。

在后续的研究中，（Ho和Salimans，2021）提出了一种名为无分类器引导的纯生成方法。通过对（4.89）进行简单重排，我们得到：

\[\nabla \overbrace{log p_{Y | t}(y | x)}^{classifier }=\overbrace{\nabla log p_{t | Y}(x | y)}^{conditional score }-\overbrace{\nabla log p_{t}(x)}^{unconditional score } \tag{4.92}\]

这表明分类器的得分可以通过 vanilla 概率路径和引导概率路径的得分之差来隐式近似。随后，作者提出使用同一模型同时学习条件得分和无条件得分。在速度方面，Zheng等人（2023）表明，也可以将4.92代入4.91，并使用表1中得分到速度的转换来得到：

\[\tilde{u}_{t}^{\theta}(x | y)=(1-w) u_{t}^{\theta}(x | \emptyset )+w u_{t}^{\theta}(x | y), \tag{4.93}\]

其中w再次是引导校准尺度。现在，只训练一个模型：\(u_{t}^{\theta}(x | y)\)，其中\(y \in{Y, \emptyset}\)，Ø是表示空条件的占位值，而\(u_{t}^{\theta}(x | \emptyset)\)是生成无条件概率路径\(p_{t}(x)\)的速度场。由此产生的损失为：

\[\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t, \xi,\left(X_{0}, X_{1}, Y\right) \sim \pi_{0,1, Y}}\left[D\left(\dot{\psi}_{t}\left(X_{0} | X_{1}\right), u_{t}^{\theta}\left(X_{t} |(1-\xi) \cdot Y+\xi \cdot \emptyset \right)\right)\right],  \tag{4.94}\]

其中ξ服从伯努利分布\(( p_{uncond })\)，\(p_{uncond }\)是训练期间抽取空条件\(\emptyset\) 的概率。CFG所采样的精确分布尚不清楚，一些研究为CFG采样提出了不同的直观或理论依据（Dieleman，2022；Guo等人，2024；Chidambaram等人，2024；Bradley和Nakkiran，2024）。尽管如此，在撰写本文时，CFG是训练条件模型最流行的方法。Esser等人（2024）、Polyak等人（2024）展示了无分类器引导在训练大规模引导FM模型中的应用。







