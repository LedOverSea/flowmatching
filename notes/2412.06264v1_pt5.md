# 9. 生成器匹配
在本节中，我们将介绍生成器匹配（GM）（Holderrieth等人，2024），这是一种生成建模框架，适用于（1）任意数据模态和（2）一般马尔可夫过程。GM统一了近年来开发的绝大多数生成模型，包括扩散模型、“离散扩散”模型以及前几节中描述的FM变体。为了介绍GM，我们在第8节中定义了CTMP生成模型，该模型通过马尔可夫过程的生成器构建而成。GM描述了一种用于训练生成器的可扩展算法——这也是该方法得名的原因。除了提供一个统一的框架外，GM还催生了多种新模型，使我们能够组合不同类别的模型，并且能够为任意模态构建模型，包括跨多种数据模态的模型。

## 9.1 数据与耦合
和之前一样，我们的目标是将样本\(X_{0} \sim p\)从分布p转换为目标分布q中的样本\(X_{1} \sim q\)，其中\(x_{0}\)、\(X_{1} \in S\)是两个随机变量，每个都在状态空间s中取值。源样本和目标样本可以通过独立耦合\((X_{0}, X_{1}) \sim p \otimes q\)（乘积分布）相关联，或者通过一般的概率质量函数耦合\(\pi_{0,1}\)、\(i.e.\)（\(S ×S\)上的分布，其边缘分布为\(\pi_{0}=p\)和\(\pi_{1}=q\)）相关联。与之前唯一的区别是，s是一个一般的状态空间，且p、q可以是任意的概率测度。

## 9.2 一般概率路径
GM方法的下一步，和之前一样，是指定一条概率路径\(p_{t}\)来插值p和q。按照4.4节的内容，我们使用条件概率路径\(p_{t | Z}(~d x | z)\)，即一组依赖于潜在状态\(z \in Z\)的时变概率测度。给定z的分布\(p_{Z}\)，我们考虑通过分层抽样过程定义的相应边际概率路径\(p_{t}(~d x)\)：

\[Z \sim p_{Z}, X_{t} \sim p_{t | Z}( d x | z) \Rightarrow X_{t} \sim p_{t}( d x)\]

也就是说，我们通过先从\(p_{Z}\)中抽样Z，然后从\(p_{t | Z}(~d x | z)\)中抽样\(X_{t}\)，来获得\(p_{t}\)的样本。和之前一样，边际概率路径的构建需满足边界约束\(p_{0}=p\)和\(p_{1}=q\)。

我们已经看到了\(Z=S\)和\(p_{Z}=q\)的两种常见构造：首先，用于\(S=\mathbb{R}^{d}\)的仿射条件流（如在连续流形中使用；第4节）通过

\[Z \sim q, X_{0} \sim p, X_{t}=\sigma_{t} X_{0}+\alpha_{t} Z \Rightarrow X_{t} \sim p_{t}( d x) (9.1)\]

来定义，其中\(\alpha_{t}\)、1、\(\sigma_{t} \in \mathbb{R}_{≥0}\)是满足\(\alpha_{0}=\sigma_{1}=0\)和\(\alpha_{1}=\sigma_{0}=1\)的可微函数。其次，对于任意的s，我们可以使用在离散状态空间的离散流形中所使用的混合（方程（7.22））：
$$Z \sim q, X_0 \sim p, X_t \sim \begin{cases} Z & \text{with prob } \kappa_t \\ X_0 & \text{with prob } (1 - \kappa_t) \end{cases} \implies X_t \sim p_t(\mathrm{d}x) \tag{9.2}$$

其中\(\kappa_{t} \in \mathbb{R}_{≥0}\)是满足\(\kappa_{0}=0\)、\(\kappa_{1}=1\)和\(0 ≤\kappa_{t} ≤1\)的可微函数。可以很容易地看出，仿射条件概率路径和混合概率路径对p和q进行插值，即\(p_{0}=p\)和\(p_{1}=q\)。

## 9.3 通过神经网络对生成器进行参数化

给定一条概率路径\(p_{t}\)，我们的目标是构建一个由生成器\(L_{t}\)指定的CTMP模型，该模型能生成这条概率路径（参见方程（8.10））。为了为此训练一个神经网络，我们首先需要说明如何用一个带有参数e的神经网络\(L_{t}^{\theta}\)来参数化生成器\(L_{t}\)。我们将在本节中完成这一工作。

设T再次为一族测试函数（见8.2.1节）。\(L_{t}\)的线性参数化定义如下：对于每个\(x \in S\)，存在（1）一个凸闭集\(\Omega_{x} \subset V_{x}\)，它是具有内积\(<\cdot, \cdot>_{x}\)的向量空间\(V_{x}\)的子集，以及（2）一个线性算子\(K: T \to C(S ; V_{x})\)，使得每个所考虑的生成元\(L_{t}\)都可以写成

\[\mathcal{L}_{t} f(x)=\left<\mathcal{K} f(x), F_{t}(x)\right>_{x} (9.3)\]

对于函数\(F_{t}\)，使得对于每个\(x \in S\)都有\(F_{t}(x) \in \Omega_{x}\)。关键在于，算子\(\mathcal{K}\)不能依赖于\(L_{t}\)，即只需要学习\(F_{t}\)。这导致了

**参数化生成器**：\(L_{t}^{\theta} f(x)=<K f(x), F_{t}^{\theta}(x)>_{x}\)，其神经网络为\(F_{t}^{\theta}\)，参数为\(\theta\)，（9.4）

其中，\(F_{t}^{\theta}\)再次将元素\(x \in S\)映射到\(F_{t}^{\theta}(x) \in \Omega_{x}\)。我们列举几个例子，以使该定义更具体。

*流的线性参数化*。令\(S=\mathbb{R}^{d}\)和\(\Omega_{x}=\mathbb{R}^{d}=V_{x}\)。我们来考虑所有的流，即生成元族由下式给出（参见方程（8.15））：

\[\mathcal{L}_{t} f=\nabla f^{T} u_{t}, u_{t}: \mathbb{R}^{d} \to \mathbb{R}^{d} . (9.5)\]

设置\(K f=\nabla f\)和\(F_{t}=u_{t}\)，我们得到式（9.3）的形式。这通过它们的向量场给出了流生成器的一种自然线性参数化。

*扩散的线性参数化*。令\(S=\mathbb{R}^{d}\)和\(\Omega_{x}=S_{d}^{++} \subset \mathbb{R}^{d ×d}=V_{x}\)，其中\(S_{d}^{++}\)表示所有半正定矩阵的集合。则扩散生成器由下式给出（参见方程（8.26））：

\[\mathcal{L}_{t} f=\nabla^{2} f \cdot \sigma_{t}^{2}, \sigma_{t}: \mathbb{R}^{d} \to S_{d}^{++} \tag{9.6}\]

设置\(\mathcal{K} f=\nabla^{2} f\)和\(F_{t}=\sigma_{t}^{2}\)，我们可以得到式（9.3）的形式。这为扩散生成器提供了一种自然的线性参数化方法。

*跳跃的线性参数化*。令\(\Omega_{x}={a: S {x} \to \mathbb{R}_{≥0} | a integrable } \subset L^{2}(S {x})=V_{x}\)具有点积\(< a, b>_{x}=\int_{S {x}} a(x) b(x) \nu(d x)\)。则跳跃生成元由下式给出（参见方程（8.36））：

\[\mathcal{L}_{t} f(x)=\int[f(y)-f(x)] Q_{t}(y, x) \nu(d y)=\left<\mathcal{K} f(x), Q_{t}(\cdot ; x)\right>_{x} (9.7)\]

其中我们将\(K f(x)\)设为函数\(y \mapsto f(y)-f(x)\)。令\(F_{t}=Q_{t}\)，我们得到方程（9.3）的形式——给出了跳跃生成元的线性参数化。我们注意到，上述内容仅对具有跳跃核\(Q_{t}(y, x)\)的跳跃进行参数化，这不一定包含所有跳跃测度。

*CTMC的线性参数化*。令s为离散的，且\(u_{t} \in \mathbb{R}^{S ×S}\)为连续时间马尔可夫链的速率矩阵。对于离散FM（参见方程（7.5）），我们定义
\[\Omega_{x}=\left\{v \in \mathbb{R}^{\mathcal{S}} | v(y) \geq 0 \forall y \neq x, and v(x)=-\sum_{y \neq x} v(y)\right\} \subset V_{x}=\mathbb{R}^{\mathcal{S}} . \tag{9.8}\]

然后根据方程（8.38），对于\(f \in \mathbb{R}^{S}\)，生成元由
\[\mathcal{L}_{t} f(x)=f^{T} u_{t}(\cdot, x)=\left< f, u_{t}(\cdot, x)\right>_{x} (9.9)\]
给出，其中\(V_{x}=\mathbb{R}^{S}\)和\(K f=f\)以及\(<\cdot, \cdot>_{x}\)是标准欧几里得点积。由此，我们得到了方程（9.3）的形式。因此，这通过它们的速率\(u_{t}\)给出了连续时间马尔可夫链（CTMCs）的一种自然线性参数化。

*流形上流的线性参数化*。设\(S=M\)为一个黎曼流形，并且如第5节所述，我们考虑黎曼流形上的流。根据方程（8.39），生成元由
\[\mathcal{L}_{t} f(x)=\left<\nabla f(x), u_{t}(x)\right>_{g} \quad(9.10)\]
给出，其中\(u_{t}\)是一个依赖于时间的光滑向量场\(u_{t}:[0,1] ×M \to T M\)：且对于所有\(x \in M\)，都有\(u_{t}(x) \in T_{x} M\)。令\(\Omega_{x}=V_{x}=T_{x} M\)和\(K=\nabla f\)为梯度算子，我们就能得到方程（9.3）的形式。因此，这给出了黎曼流生成元的一种自然的线性参数化。

## 9.4 边际生成器和条件生成器
在本节中，我们将展示如何找到边际概率路径的生成元。方法如下：我们可以找到条件概率路径\(p_{t | Z}(~d x | z)\)的生成元，这通常可以通过解析方法实现，并利用这些生成元来构造边际路径的生成元。具体来说，假设对于每个\(z \in Z\)，我们都找到了一个（条件）生成元\(C^{2}\)，它生成\(p_{t | Z}(~d x | z)\)，即根据定理17，这等价于柯尔莫哥洛夫前向方程（式（8.40））：

$$\frac{\mathrm{d}}{\mathrm{d} t}\langle p_{t|z}(\cdot|z), f \rangle = \langle p_{t|z}(\cdot|z), \mathcal{L}_{t}^{*} f \rangle \quad \text{for all } f \in \mathcal{T}. \tag{9.11}$$

此外，我们假设找到了如下的线性参数化（参见方程（9.3））：
$$\mathcal{L}_{t}^{*} f(x)=\langle\mathcal{K}(f, x), F_{t}(x, z)\rangle_{z} \quad z \in \mathcal{Z} \tag{9.12}$$

对于函数\(F_{t}(x | z) \in \Omega_{x} \subset V_{x}\)而言。例如，\(F_{t}(x | z)\)可以是连续流形（FM）中的条件速度场（参见4.3节），也可以是离散流形中的条件速率（参见公式（7.2））。这使我们能够找到生成边际路径的生成器公式：

**定理 19（一般边缘化技巧）**：边际概率路径\(\{p_t\}_{0\leq t \leq 1}\)由具有生成元的马尔可夫过程\(x_t\)生成。

$$\mathcal{L}_{t} f(x)=\mathbb{E}_{Z \sim p_{Z \mid t}(z \mid x)}\left[\mathcal{L}_{t}^{Z} f(x)\right] \tag{9.13}$$

其中\(p_{Z \mid t}(z \mid x)\)是后验分布（即给定\(x\)时\(z\)的条件分布）。生成器\(-\mathcal{L}_{t}\)具有由下式给出的线性参数化形式

$$F_{t}(x)=\mathbb{E}_{Z \sim p_{Z \mid t}(z \mid x)}\left[F_{t}(x \mid Z)\right]. \tag{9.14}$$

上述定理为我们提供了训练目标：用神经网络逼近方程（9.13）中的\(L_{t}\)。前几章中提到的边缘化技巧（定理3、定理10、定理14）都是该定理的特例。我们在此给出证明，然后展示几个新颖的实例。

证明：为了证明\(\mathcal{L}_t\)生成\(p_t\)，我们需要根据定理17证明满足KFE。设\(p_{t+h}(\cdot|x,\mathcal{Z})\)为\(\mathcal{L}_t^Z\)的转移核。则有：

$$
\begin{aligned}
\frac{\mathrm{d}}{\mathrm{d} t} \langle p_t, f \rangle &= \lim_{h \to 0} \frac{1}{h} \left( \langle p_{t+h}, f \rangle - \langle p_t, f \rangle \right) \\
&= \lim_{h \to 0} \frac{1}{h} \left( \mathbb{E}_{Z \sim p_{Z}, X \sim p_{t+h}|z(\cdot|Z)}[f(X')] - \mathbb{E}_{Z \sim p_{Z}, X \sim p_{t}|z(\cdot|Z)}[f(X)] \right) \\
&= \lim_{h \to 0} \frac{1}{h} \left( \mathbb{E}_{Z \sim p_{Z}, X \sim p_{t}|z(\cdot|Z), X' \sim p_{t+h}|z(\cdot|X,Z)}[f(X')] - \mathbb{E}_{Z \sim p_{Z}, X \sim p_{t}|z(\cdot|Z)}[f(X)] \right) \\
&= \lim_{h \to 0} \frac{1}{h} \left( \mathbb{E}_{Z \sim p_{Z}, X \sim p_{t}|z(\cdot|Z), X' \sim p_{t+h}|z(\cdot|X,Z)}[f(X') - f(X)] \right) \\
&= \lim_{h \to 0} \frac{1}{h} \mathbb{E}_{X \sim p_t} \left( \mathbb{E}_{Z \sim p_{Z|t}(\cdot|X)} \left( \mathbb{E}_{X' \sim p_{t+h}|z(\cdot|X,Z)}[f(X')] - f(X) \right) \right) \\
&= \mathbb{E}_{X \sim p_t} \left( \mathbb{E}_{Z \sim p_{Z|t}(\cdot|X)} \left( \lim_{h \to 0} \frac{1}{h} \left( \mathbb{E}_{X' \sim p_{t+h}|z(\cdot|X,Z)}[f(X')] - f(X) \right) \right) \right) \\
&= \mathbb{E}_{X \sim p_t} \left( \mathbb{E}_{Z \sim p_{Z|t}(\cdot|X)} \left( \mathcal{L}_t^Z f(X) \right) \right) \quad \text{（记为}\mathcal{L}_t f(X)\text{）} \\
&= \langle p_t, \mathcal{L}_t f \rangle
\end{aligned}
$$

那么，\(F_t\)形式的证明如下。

$$
\begin{aligned}
\mathbb{E}_{Z \sim p_{Z|t}(\cdot|X)}(\mathcal{L}_t^Z f(X)) &\stackrel{(9.12)}{=} \mathbb{E}_{Z \sim p_{Z|t}(\cdot|X)} \left( \langle \mathcal{K}(f, X), F_t(X|Z) \rangle_z \right) \\
&= \langle \mathcal{K}(f, X), \mathbb{E}_{Z \sim p_{Z|t}(\cdot|X)}[F_t(X|Z)] \rangle_z \\
&= \langle \mathcal{K}(f, X), F_t(X) \rangle_z
\end{aligned}
$$

在这里，我们利用点积的线性性质将其与期望值交换。这表明\(F_t\)是边缘生成器的线性参数化（参见方程(9.3)）。 ■

*示例 - 跳跃*。令s为任意值，\(Q_{t}(y, x | z)\)为s上针对y、\(x \in S\)、\(z \in Z\)的条件跳跃核，它生成条件概率路径\(p_{t | Z}(~d x | z)\)。利用跳跃核的线性参数化（参见方程（9.7）），我们得到边际跳跃核

\[Q_{t}(y, x)=\mathbb{E}_{Z \sim p_{Z | t}(\cdot | x)}\left[Q_{t}(y, x | z)\right] .\]

生成边际概率\(p_{t}(~d x)\)。

*示例 — 边际扩散系数*。设\(S=\mathbb{R}^{d}\)和\(\sigma_{t}^{2}(x | z)\)为生成条件概率路径\(p_{t | Z}(~d x | z)\)的扩散系数。利用扩散系数的线性参数化（参见方程（9.6）），我们得到边际扩散系数
\[\sigma_{t}^{2}(x)=\mathbb{E}_{Z \sim p_{Z | t}(\cdot | x)}\left[\sigma_{t}^{2}(x | Z)\right]\]
生成边际概率路径\(p_{t}(~d x)\)。

## 9.5 生成器匹配损失
我们的下一个目标是为学习CTMP模型制定一个训练目标。假设我们有一个神经网络\(F_{t}^{\theta}\)，它能为我们提供如公式（9.4）所示的生成器参数化\(L_{t}^{\theta}\)。正如定理19中所推导的，我们的目标是逼近公式（9.14）给出的真实边际线性参数化\(F_{t}\)。和之前一样，假设对于每个\(x \in S\)，我们都有一个通过
\[D_{x}(a, b)=\Phi_{x}(a)-\left[\Phi_{x}(b)+\left< a-b, \nabla \Phi_{x}(b)\right>\right], a, b \in \Omega_{x} (9.15)\]
定义的布雷格曼散度\(D_{x}: \Omega_{x} ×\Omega_{x} \to \mathbb{R}\)，其中涉及一个严格凸函数\(\Phi_{x}: \Omega_{x} \to \mathbb{R}\)（见图10）。用于训练CTMP模型的生成器匹配损失定义为
\[\mathcal{L}_{GM}(\theta)=\mathbb{E}_{t, X_{t} \sim p_{t}} D_{X_{t}}\left(F_{t}\left(X_{t}\right), F_{t}^{\theta}\left(X_{t}\right)\right), \quad(9.16)\]
，其中\(t ~ U[0,1]\)。遗憾的是，上述训练目标难以处理，因为我们既不知道边际生成器\(L_{t}\)，也不知道其参数化\(F_{t}\)（我们只知道公式（9.14）中那个难以处理的公式）。因此，我们引入条件生成器匹配损失作为一种易于处理的替代方案，其形式为
\[\mathcal{L}_{CGM}(\theta)=\mathbb{E}_{t, Z, X_{t} \sim p_{t | Z}} D_{X_{t}}\left(F_{t}\left(X_{t} | Z\right), F_{t}^{\theta}\left(X_{t}\right)\right) . (9.17)\]

这一目标是易于处理的，因为在许多情况下，我们可以通过解析推导出\(F_{t}(x | z)\)（见9.6节）。如下一定理所示，两种损失（9.16）和（9.17）都提供相同的学习梯度。

**定理20**. 生成器匹配损失和条件生成器匹配损失的梯度一致：

\[\nabla_{\theta} \mathcal{L}_{G M}(\theta)=\nabla_{\theta} \mathcal{L}_{C G M}(\theta) . (9.18)\]

特别是，条件生成器匹配损失的极小值是边际生成器的线性参数化（式（9.14））：

\[F_{t}^{\theta}(x)=\mathbb{E}_{Z \sim p_{Z | t}(\cdot | x)}\left[F_{t}(x | Z)\right] . (9.19)\]

此外，要使这些性质成立，\(D_{x}\)必然是一种布雷格曼散度。


上述定理将之前章节中推导的定理 4、定理 11 和定理 15 推广到了一般的 CTMP（连续时间马尔可夫过程）模型。它允许我们通过最小化条件生成器匹配损失，以可扩展的方式轻松训练任何由神经网络 \( F_t^\theta \) 参数化的 CTMP 模型。此外，它还普遍刻画了损失函数的空间。定理 20 的证明与定理 4 的证明相同，只是用 \( F_t \) 代替了 \( u_t \)。关于 \( D \) 必须是 Bregman 散度的必要性证明，我们参考（Holderrieth et al., 2024）。

*示例——训练扩散系数*。我们将说明定理20如何让我们训练随机微分方程（SDE）的扩散系数。设\(S=\mathbb{R}^{d}\)和\(\sigma_{t}^{2}(x | z)\)为生成条件概率路径\(p_{t | Z}(~d x | z)\)的扩散系数。我们可以在神经网络\((\sigma_{t}^{2})^{\theta}(x) \in \mathbb{R}^{d ×d}\)中对扩散系数进行参数化。此时，条件生成器匹配损失变为
\[\mathcal{L}_{CGM}(\theta)=\mathbb{E}_{t, Z, X_{t} \sim p_{t | Z}}\left\| \sigma_{t}^{2}\left(X_{t} | Z\right)-\left(\sigma_{t}^{2}\right)^{\theta}\left(X_{t}\right)\right\| ^{2}\]
，其中我们使用均方误差作为布雷格曼散度（还有许多其他可选方法）。在（Holderrieth等人，2024）中，展示了以此方式训练的模型示例。

## 9.6 找到作为KFE解的条件生成器 
为了使用条件生成器匹配损失实现可扩展训练（参见定理20），我们需要能够找到一个条件生成器\(L_{t}^{z}\)来求解KFE 
\[\frac{d}{d t}\left< p_{t | Z}(\cdot | z), f\right>=\left< p_{t | Z}(\cdot | z), \mathcal{L}_{t}^{z} f\right> for all f \in \mathcal{T}, z \in \mathcal{Z} . (9.20)\]

如果\(p_{t | Z}(~d x | z)\)相对于2具有密度\(p_{t | Z}(x | z)\)。在这种情况下，我们可以等效地求解伴随KFE 
\[\frac{d}{d t} p_{t | Z}(x | z)=\left[\left(\mathcal{L}_{t}^{z}\right)^{*} p_{t | Z}(\cdot | z)\right](x) for all x \in \mathcal{S}, z \in \mathcal{Z} . (9.21)\]

总的来说，方程（9.20）和方程（9.21）是难以通过解析方法求解的方程，对于任意生成元，没有通用的求解公式。因此，我们给出2个例子来说明如何进行求解。

我们在此用跳跃模型对其进行说明，因为这些模型适用于任意状态空间。如8.2.2节所述，它们由一个跳跃测度\(Q_{t}\)来指定，该测度可分解为


\[Q_{t}( d y, x)=\lambda_{t}(x) J_{t}( d y, x) for all x \in \mathcal{S} \tag{9.22}\]



\[\lambda_{t}(x) \geq 0 for all x \in \mathcal{S} \tag{9.23}\]



\[\int J_{t}(d y, x)=1 for all x \in \mathcal{S} \tag{9.24}\]


其中，\(\lambda_{t}(x)\)描述跳跃强度，\(J_{t}\)描述一个指定跳跃分布的概率核。请注意，为简化符号，我们省略了对\(z \in Z\)的依赖（为避免与边际概率路径混淆，我们对\(p_{t | Z}(~d x | z)\)保留了这一依赖）。

*凸混合的跳跃模型*。让我们考虑由（参见方程（7.22））给出的混合概率路径：

\[p_{t | Z}( d x | z)=\kappa_{t} \delta_{z}( d x)+\left(1-\kappa_{t}\right) p( d x), z \in \mathcal{S} . (9.25)\]

利用跳跃过程生成器的形式（参见方程（8.36）），KFE变为：

\[\frac{d}{d t}\left< p_{t | Z}( d x | z), f\right>=\mathbb{E}_{X \sim p_{t | Z}(\cdot | z)} \lambda_{t}(X) \mathbb{E}_{Y \sim J_{t}( d y, x)}[f(Y)-f(X)] for all f \in \mathcal{T}, x \in \mathcal{S} (9.26)\]

其中\(\lambda_{t}\)、\(J_{t}\)满足方程（9.23）和方程（9.24）中的约束条件。我们认为，对于具有以下条件的跳跃模型，这一点是成立的：

\[Q_{t}( d y, x)=\lambda_{t}(x) J_{t}( d y, x), \lambda_{t}(x)=\frac{\dot{\kappa}_{t}}{1-\kappa_{t}}, J_{t}( d y, x)=\delta_{z}( d y)\]

即，跳跃强度由\(\lambda_t\)给出，并且一旦我们决定跳跃，就直接跳到\(z \in S\)。为了说明这一点，我们证明上述跳跃过程满足KFE（柯尔莫哥洛夫向前方程）。我们可以推导：

$$
\begin{aligned}
\mathbb{E}_{X \sim p_{t \mid Z}(z \mid \cdot)} \left[ \lambda_t(X) \mathbb{E}_{Y \sim J_t(X)} [f(Y) - f(X)] \right] &= \frac{\kappa_t}{1 - \kappa_t} \mathbb{E}_{X \sim p_{t \mid Z}(z \mid \cdot)} [f(z) - f(X)] \\
&= \frac{\kappa_t}{1 - \kappa_t} \left[ f(z) - \mathbb{E}_{X \sim p_{t \mid Z}(z \mid \cdot)} [f(X)] \right] \\
&= \frac{\kappa_t}{1 - \kappa_t} \left[ f(z) - \left[ \kappa_t f(z) + (1 - \kappa_t) \mathbb{E}_{X \sim p} f(X) \right] \right] \\
&= \kappa_t f(z) - \kappa_t \mathbb{E}_{X \sim p} f(X) \\
&= \frac{\mathrm{d}}{\mathrm{d} t} \left[ \kappa_t f(z) + (1 - \kappa_t) \mathbb{E}_{X \sim p} f(X) \right] \\
&= \frac{\mathrm{d}}{\mathrm{d} t} \mathbb{E}_{X \sim p_{t \mid Z}(z \mid \cdot)} [f(X)] \\
&= \frac{\mathrm{d}}{\mathrm{d} t} \langle p_{t \mid Z}(z \mid \cdot), f \rangle.
\end{aligned}
$$

因此，我们可以看出该过程满足跳跃KFE（方程（9.26））。由此，我们建立了一个跳跃模型。我们已经在方程（7.24）中看到了该模型在离散状态空间下的一个特殊示例。例如，在这里我们已经证明，也可以为欧几里得空间\(\mathbb{R}^{d}\)构建类似的跳跃模型。

*具有密度的任意路径的跳跃模型*。假设我们有一个概率\(p_{t | Z}(~d x | z)\)，它关于s上的参考测度\(24\)具有密度\(p_{t | Z}(x | z)\)，并且在t中是可微的（请注意，方程（9.25）中的混合路径对于\(S=\mathbb{R}^{d}\)不满足这一点）。此外，我们将自己限制在具有密度的跳跃核\(J_{t}(y, x)\)上。由此，伴随KFE成为跳跃连续性方程（方程（8.66））：

\[\frac{d}{d t} p_{t | Z}(x | z)=\int \lambda_{t}(y) J_{t}(x, y) p_{t | Z}(y | z) d y-p_{t | Z}(x | z) \lambda_{t}(x) \tag{9.27}\]



\[\Leftrightarrow p_{t | Z}(x | z)\left[\frac {d}{dt}log p_{t| Z}(x| z)+\lambda _{t}(x)\right] =\int \lambda_{t}(y) J_{t}(x,y)p_{t| Z}(y | z) d y \tag{9.28}\]

制作\(J_{t}(x, y)=J_{t}(x)\)（“目标状态无关”）并使用\(\partial_{t}=\frac{d}{d t}\)，我们得到这等价于：

\[p_{t | Z}(x | z)\left[\partial_{t} log p_{t | Z}(x | z)+\lambda_{t}(x)\right]=J_{t}(x) \int \lambda_{t}(y) p_{t | Z}(y | z) \nu(d y) (9.29)\]



\[\Leftrightarrow \frac{p_{t | Z}(x | z)\left[\partial_{t} log p_{t | Z}(x | z)+\lambda_{t}(x)\right]}{\int \lambda_{t}(y) p_{t | Z}(y | z) \nu(d y)}=J_{t}(x) (9.30)\]

为了定义一个有效的跳跃过程，我们要求\(\lambda_{t}\)、\(J_{t}\)满足\(\lambda_{t}(x) ≥0\)、\(J_{t}(x) ≥0\)。因此，我们得到：

\[\lambda_{t}(x) \geq 0, J_{t}(x) \geq 0 \Leftrightarrow \lambda_{t}(x) \geq\left[-\partial_{t} log p_{t}(x | z)\right]_{+} (9.31)\]

其中\([x]_{+}=max (x, 0)\)描述ReLU运算。此外，我们要求\(J_{t}\)定义一个有效的跳跃分布，即积分结果为1。可以看出这一点是成立的：

\[\begin{aligned} 1 & =\int J_{t}(x) d x \\ \Leftrightarrow \int \lambda_{t}(x) p_{t | Z}(x | z) \nu(d x) & =\int p_{t | Z}(x | z)\left[\partial_{t} log p_{t | Z}(x | z)+\lambda_{t}(x)\right] \nu(d x) \\ \Leftrightarrow 0 & =\int \partial_{t} p_{t | Z}(x | z) \nu(d x) \\ \Leftrightarrow 0 & =\partial_{t} \int p_{t | Z}(x | z) \nu(d x) \\ \Leftrightarrow 0 & =0 \end{aligned}\]

即，\(J_{t}\) 确实积分为 1 。选择最小的 \(\lambda_{t}(x)\)，我们得到通过以下方式定义的跳跃模型

$$
\begin{aligned}
\lambda_{t}(x) &= \left[ -\partial_{t} \log p_{t \mid Z}(x \mid z) \right]_{+}, \\
J_{t}(x) &= \frac{p_{t \mid Z}(x \mid z)\left[\partial_{t} \log p_{t \mid Z}(x \mid z)\right]_{+}}{\int p_{t \mid Z}(y \mid z)\left[\partial_{t} \log p_{t \mid Z}(y \mid z)\right]_{+} \nu(\mathrm{d} y)} = \frac{\left[\partial_{t} p_{t \mid Z}(x \mid z)\right]_{+}}{\int\left[\partial_{t} p_{t \mid Z}(y \mid z)\right]_{+} \nu(\mathrm{d} y)}.
\end{aligned}
$$

是跳跃连续性方程的一个解，因此生成了条件概率路径\(p_{t | Z}(x | z)\)。起初，跳跃分布与位置无关这一点似乎并不令人满意。然而，如果我们将该模型扩展到多个维度，跳跃分布将取决于位置，并由此产生一个强大的生成模型（Holderrieth等人，2024年）。

## 9.7 模型组合
在本节中，我们将解释生成匹配（GM）如何让我们以不同方式组合针对同一状态空间\( S \)的生成模型。基本原理很简单：生成器是一个线性算子，而柯尔莫哥洛夫向前方程（KFE）\(\partial_{t}\langle p_{t}, f \rangle = \langle p_{t}, \mathcal{L}_{t} f \rangle\)是一个线性方程——所以从本质上讲，我们可以像在线性代数中处理矩阵方程那样，组合这个方程的解。具体来说，设\(\mathcal{L}_{t}\)、\(\mathcal{L}_{t}'\)是两个马尔可夫过程的生成器，这两个马尔可夫过程都为概率路径\( p_{t} \)满足KFE。那么，对于\(\alpha_{t}^{1}, \alpha_{t}^{2} \in \mathbb{R}\)且\(\alpha_{t}^{1} + \alpha_{t}^{2} = 1\)，有：

$$\langle p_{t}, \alpha_{t}^{1}\mathcal{L}_{t} + \alpha_{t}^{2}\mathcal{L}_{t}' f \rangle \tag{9.32}$$

$$= \alpha_{t}^{1}\langle p_{t}, \mathcal{L}_{t} f \rangle + \alpha_{t}^{2}\langle p_{t}, \mathcal{L}_{t}' f \rangle \tag{9.33}$$

$$= \alpha_{t}^{1}\partial_{t}\langle p_{t}, f \rangle + \alpha_{t}^{2}\partial_{t}\langle p_{t}, f \rangle \tag{9.34}$$

$$= (\alpha_{t}^{1} + \alpha_{t}^{2})\partial_{t}\langle p_{t}, f \rangle \tag{9.35}$$

$$= \partial_{t}\langle p_{t}, f \rangle, \tag{9.36}$$

即\(\alpha_{t}^{1}\mathcal{L}_{t} + \alpha_{t}^{2}\mathcal{L}_{t}'\)也是KFE的一个解。一个小但重要的细节是\(\alpha_{t}^{1}\)、\(\alpha_{t}^{2}\)是否为正或负，以及\(\mathcal{L}_{t}\)、\(\mathcal{L}_{t}'\)对应于正向还是反向时间的马尔可夫过程。这就引出了下面的内容。

**命题 3（模型组合）**：设\( p_t \)为边际概率路径，那么以下生成器满足柯尔莫哥洛夫向前方程（KFE），并共同定义一个以\( p_t \)为边际的生成模型：
1. **马尔可夫叠加**：\(\alpha_t^1 \mathcal{L}_t + \alpha_t^2 \mathcal{L}_t'\)，其中\(\mathcal{L}_t\)、\(\mathcal{L}_t'\)是解决\( p_t \)的 KFE 的马尔可夫过程的两个生成器，且\(\alpha_t^1, \alpha_t^2 \geq 0\)满足\(\alpha_t^1 + \alpha_t^2 = 1\)。
2. **无散度分量**：\(\mathcal{L}_t + \beta_t \mathcal{L}_t^{\text{div}}\)，其中\(\mathcal{L}_t^{\text{div}}\)是一个生成器，满足对所有\( f \in \mathcal{T} \)，\(\langle p_t, \mathcal{L}_t^{\text{div}} f \rangle = 0\)，且\(\beta_t \geq 0\)。我们称此类\(\mathcal{L}_t^{\text{div}}\)为无散度的。
3. **预测 - 校正**：\(\alpha_t^1 \mathcal{L}_t - \alpha_t^2 \mathcal{L}_t^*\)，其中\(\mathcal{L}_t\)是一个在正向时间解决\( p_t \)的 KFE 的生成器，\(\mathcal{L}_t^*\)是一个在反向时间解决 KFE 的生成器，且\(\alpha_t^1, \alpha_t^2 \geq 0\)满足\(\alpha_t^1 - \alpha_t^2 = 1\)。

我们在此给出马尔可夫叠加和无散度分量的例子，以说明命题3。关于预测-校正格式的效力，可参见（Gat等人，2024）中的一个例子。

**马尔可夫叠加示例——结合跳跃与流**：马尔可夫叠加可用于组合不同类别的生成模型。这些模型可以是分别训练的网络，也可以是在一个网络中同时训练的两个GM（生成匹配）模型（Holderrieth等人，2024）。在这里，我们通过将一个跳跃模型和一个流模型组合在\( S = \mathbb{R}^d \)上，来展示这一点。假设我们有两个模型，每个模型都生成概率路径\( p_t \)：（1）一个流模型，其生成器为\( \mathcal{L}_t^{\text{flow}} \)；（2）一个跳跃模型，具有跳跃强度\( \lambda_t \)和跳跃分布\( J_t \)。根据命题3，对于\( \alpha_t^1, \alpha_t^2 \geq 0 \)且\( \alpha_t^1 + \alpha_t^2 = 1 \)，以下生成器定义了一个生成\( p_t \)的有效GM模型：

$$
\begin{aligned}
\mathcal{L}_t f(x) &= \alpha_t^1 \mathcal{L}_t^{\text{jump}} f(x) + \alpha_t^2 \mathcal{L}_t^{\text{flow}} f(x) \\
&= (\alpha_t^1 \lambda_t(x))\mathbb{E}_{Y \sim J_t(x)} [f(Y) - f(x)] + \nabla f(x)(\alpha_t^2 u(x))
\end{aligned}
$$

其中，我们使用了方程(8.17)和方程(8.36)。事实上，上述生成器描述了一个分段确定性马尔可夫过程，一种ODE（常微分方程）与跳跃模型的组合（Davis，1984）。如上述方程所示，我们必须将跳跃强度按\(\alpha_t^1\)缩放，将向量场按\(\alpha_t^2\)缩放。我们可以通过以下采样过程从所得的GM模型中采样：

\[
\begin{align*}
X_0 &\sim p_0 = p \\
X_{t+h} &=
\begin{cases}
\sim J_t(\mathrm{d}y, X_t) & \text{with probability } h\alpha_t^1\lambda_t(X_t) \\
X_t + h\alpha_t^2 u(X_t) & \text{with probability } 1 - h\alpha_t^1\lambda_t(X_t)
\end{cases}
\end{align*}
\]

在（Holderrieth等人，2024）中，给出了几个跳跃与流的马尔可夫叠加的例子，并表明这会带来性能提升。

*无散度示例——MCMC算法*：为了找到无散度分量，可以使用现有的马尔可夫链蒙特卡罗（MCMC）算法——所有这些算法都描述了寻找无散度分量的通用方法。我们用两个著名的例子来说明这一点。假设我们有一个密度为\( p_t(x) \)的一般概率路径\( p_t \)。那么，生成器\(\mathcal{L}_t^{\text{div}}\)是无散度的，等价于其伴随将\( p_t \)映射到零：

\[
\langle p_t, \mathcal{L}_t^{\text{div}} f \rangle = 0 \text{ 对所有 } f \in \mathcal{T} \iff [\mathcal{L}_t^{\text{div}}]^* p_t(x) = 0 \text{ 对所有 } x \in S \tag{9.37}
\]

首先，考虑\( S = \mathbb{R}^d \)。朗之万动力学对应于具有速度场\(\frac{1}{2}\beta^2 \nabla \log p_t(x)\)和扩散系数\(\beta_t\)的随机微分方程（SDE），即，通过以下方式给出的动力学：

\[
dX_t = \frac{1}{2}\beta^2 \nabla \log p_t(x) dt + \beta_t dW_t \tag{9.38}
\]

该SDE的伴随生成器由下式给出：

\[
\begin{align*}
[\mathcal{L}_t^{\text{div}}]^* p_t &\stackrel{(i)}{=} -\text{div}\left(p_t \frac{1}{2}\beta^2 \nabla \log p_t(x)\right) + \frac{1}{2}\beta_t^2 \Delta p_t(x) \\
&\stackrel{(ii)}{=} -\frac{1}{2}\text{div}\left(p_t \beta^2 \nabla h_t(x)\right) + \frac{1}{2}\beta_t^2 \Delta p_t(x) \\
&\stackrel{(iii)}{=} -\frac{1}{2}\beta_t^2 \Delta p_t(x) + \frac{1}{2}\beta_t^2 \Delta p_t(x) = 0
\end{align*}
\]

其中（i）由第 8.3.1 节推导的流和扩散伴随的形式成立，（ii）成立是因为\(\nabla \log p_t = \nabla p_t / p_t\)，且（iii）成立是由于恒等式\(\text{div} \nabla = \Delta\)。上述内容表明朗之万动力学的生成器满足方程（9.37），因此是无散度的，这符合命题 3 的意义。这一事实在统计物理学和马尔可夫链蒙特卡罗（Roberts 和 Tweedie，1996）中被广泛应用。命题 3 表明，对于任意\(\beta_t \geq 0\)，我们可以将这些动力学添加到任何生成模型中。在第 10 节中，我们利用这一点推导出扩散模型的随机采样方法。

其次，再次令\( S \)为一般状态空间。Metropolis-Hastings 算法（Hastings，1970）描述了一个具有跳跃核\( Q_t(y, x) \)的跳跃过程的构造，该跳跃核满足细致平衡条件：

\[
Q_t(y, x)p_t(x) = Q_t(x, y)p_t(y) \quad \text{对所有 } x, y \in S
\]

\[
\implies [\mathcal{L}_t^{\text{div}}]^* p_t(x) \stackrel{(i)}{=} \int Q_t(y, x)p_t(x) - Q_t(x, y)p_t(y) = 0
\]

其中（i）我们使用了方程（8.66）。这表明方程（9.37）成立，且\( Q_t \)是无散度的。命题 3 表明，可以将这样的 Metropolis 方案任意添加到遵循概率路径\( p_t \)的任何 GM 模型中。

## 9.8 多模态模型
最后，我们来讨论生成模型（GM）如何支持联合构建多种数据模态的生成模型。例如，这可以是一个同时生成图像和对应文本描述的模型。两种模态被表示为两个状态空间\(S_{1}\)、\(S_{2}\)（例如，1 \(S_{1}\)是图像，\(S_{2}\)是文本），而多模态模型则是乘积空间\(S=S_{1} ×S_{2}\)上的生成模型。由于s只是另一个状态空间，且GM适用于任意状态空间，我们可以像构建其他任何GM模型一样来构建它。不过，有一种特定的概率路径构造方法，允许我们重用或“回收”为单个模态构建的GM模型。例如，我们可以通过组合离散和连续的流模型（FM）来构建一个联合文本-图像模型。这种特定的构造依赖于因子化的条件概率路径。在7.5.2节中，我们已经见过离散流模型的一个简单案例，其中因子化的概率路径会产生因子化的速度。这一点更普遍地适用于任意模态。虽然这种构造相当简单直观，但要完全一般性地表达出来却颇具技术性。关于严格的处理，我们参考（Holderrieth等人，2024）。（Campbell等人，2024）中也实现了这种构造的一个具体实例，用于多模态蛋白质生成。这表明，GM能够以一种有原则且严谨的方式支持多模态模型的构建。

# 10. 与扩散模型及其他去噪模型的关系
在本节中，我们最后讨论去噪扩散模型（DDMs）与非欧几里得空间上相关模型的关系。我们主要关注（Song等人，2021年）通过随机微分方程（SDEs）构建的扩散模型，并解释其如何被纳入流形模型/生成模型（FM/GM）家族中。在本节末尾，我们还将讨论从去噪扩散模型（“去噪模型”）中获得灵感的其他模态模型，以及它们如何被构建为生成模型。


## 10.1 时间约定
去噪扩散模型和流匹配之间的第一个简单差异在于时间的参数化方式。这只是一种约定，但对于避免混淆很重要。与流匹配（FM）不同，扩散模型中的时间是反转的，范围从0到\(+\infty\)。为了区分这两种时间参数化，我们对扩散模型的时间约定使用\( r \)，对流匹配的时间约定使用\( t \)。于是我们有：

- 流匹配时间\( t \)：噪声\(\equiv\)“\( t = 0 \)”，数据\(\equiv\)“\( t = 1 \)” (10.1）
- 扩散时间\( r \)：噪声\(\equiv\)“\( r = +\infty \)”，数据\(\equiv\)“\( r = 0 \)” (10.2)
- 重新参数化：\( r = k(t) \)，\( t = k^{-1}(r) \) (10.3)

其中\( k: [0,1] \to [0,+\infty) \)是某个严格单调递减的映射，满足\( k(1) = 0 \)且\( \lim_{t \to 0} k(t) = +\infty \)。

## 10.2 正向概率路径
去噪扩散模型的核心思想是构建一个破坏数据分布的前向过程。我们将解释这如何对应于流形（FM）中使用的特定概率路径构造。前向过程\(X_{r}\)通过以下随机微分方程（SDE）定义：

\[d X_{r}=a_{r}\left(X_{r}\right) d r+g_{r} d W_{r}, X_{0} \sim q\]

其中，q是数据分布，\(W_{r}\)是布朗运动，\(a: \mathbb{R} ×\mathbb{R}^{d} \to \mathbb{R}^{d}\)是速度场，在随机微分方程（SDE）的语境中也称为\(drift\)，\(g: \mathbb{R} \to \mathbb{R}_{≥0}\)是扩散系数（参见8.2.2节）。每个这样的随机微分方程（SDE）都定义了一个条件概率路径和边际概率路径，如下所示：

\[\tilde{p}_{r | 0}(x | z)=\mathbb{P}\left[X_{t}=x | X_{0}=z\right], \tilde{p}_{r}(x)=\mathbb{P}\left[X_{t}=x\right]\]

\[p_{t | 1}(x | z)=\tilde{p}_{k(t) | 0}(x | z), p_{t}(x)=\tilde{p}_{k(t)}(x)\]

其中，在第二行中，我们将时间重新参数化为流形（FM）的时间参数化形式。我们可以看到，\(p_{t | 1}(x | z)\)给出了一个条件概率路径。此外，前向过程的构造使得当\(R \gg 0\)时，\(X_{R}\)的分布近似为高斯分布。因此，我们得到：

扩散中的每个前向过程都定义了一条在流匹配（FM）中使用的“有效”条件概率路径，即对应的边际路径在数据分布 \( q \) 和（大致上的）高斯分布 \( p \) 之间进行插值。具体而言：
1. **确定性初始化**：条件概率路径 \( p_{t|1}(z|z) \) 对应于当初始化为 \( X_0 = z \) 时前向过程的 SDE（随机微分方程）的分布。
2. **数据初始化**：边际概率路径 \( p_0(x) \) 对应于当初始化为 \( X_0 \sim q \)（其中 \( q \) 是数据分布）时前向过程的分布。

扩散模型的一个重要要求是，能够以闭式计算条件概率 \( p_{t|0}(x|z) \)。这就要求使用的随机微分方程（SDE）对其各自的柯尔莫哥洛夫向前方程（即福克 - 普朗克方程）具有解析解。因此，在大部分文献中，前向过程被假定具有仿射漂移系数，即 \( a_t(x) = a_t x \)，其中 \( a_t: \mathbb{R} \to \mathbb{R} \) 是某个连续函数（Song 等人，2021；Karras 等人，2022）。这一假设使我们能够将给定 \( X_0 = z \in \mathbb{R}^d \) 时 \( X_t \) 的条件分布表示为高斯分布（Särkkä 和 Solin，2019；Song 等人，2021；Karras 等人，2022）：

$$
\tilde{p}_{r \mid 0}(x \mid z)=\mathcal{N}\left(\tilde{\alpha}_{r} z, \tilde{\sigma}_{r}^{2} I\right), \quad \tilde{\alpha}_{r}=\exp \left(\int_{0}^{r} a_{w} \mathrm{d} w\right), \quad \tilde{\sigma}_{r}^{2}=\tilde{\sigma}_{0}^{2} \int_{0}^{r} \frac{a_{w}^{2}}{\tilde{\alpha}_{w}^{2}} \mathrm{d} w \tag{10.7}
$$

$$
\implies p_{t \mid 1}(x \mid z)=\mathcal{N}\left(\alpha_{t} z, \sigma_{t}^{2} I\right) \quad \alpha_{t}=\tilde{\alpha}_{k(t)}, \quad \sigma_{t}=\tilde{\sigma}_{k(t)} \tag{10.8}
$$

注意，我们在第 4.8.3 节中已将此类概率路径作为仿射高斯概率路径进行了广泛讨论，即它们是通过仿射条件流构建的（见第 4.8 节）：

$$
\psi_{t}(x \mid z)=\alpha_{t} z + \sigma_{t} x, \quad z \sim q, x \sim \mathcal{N}\left(m_{0}, \sigma_{0}^{2} I\right) \tag{10.9}
$$

因此，我们可以看出：

具有仿射漂移系数的前向过程对应于使用由方程 (10.8) 定义的高斯概率路径（见第 4.8.3 节）。

注意，对于上述时间参数化下的扩散模型，不存在有限时间 \( r < +\infty \)，使得边际 \(\tilde{p}_{r}(z)\) 是精确的高斯分布。

## 10.3 训练扩散模型
现在我们讨论如何将扩散模型的训练算法作为流模型（FM）训练的一个特例来推导。在4.8节中，我们讨论了几种用高斯概率路径对FM模型进行参数化和训练的方法（见定理7）。其中一种特定的方法是\( x_0 \)预测，即训练神经网络\( x_{0|t}^\theta \)来近似
\[
x_{0|t}^\theta \approx \mathbb{E}[X_0|X_t = x]
\]
通过以下训练算法：
\[
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,Z\sim\mathcal{N},X_0\sim p}\|x_{0|t}^\theta(\alpha_t X_0 + \sigma_t Z) - X_0\|^2
\]
（此处\( X_t \)是\( \alpha_t X_0 + \sigma_t Z \)）
\[
\stackrel{(i)}{=} \mathbb{E}_{t,Z\sim\mathcal{N},X_t\sim p_{t}}\left[\frac{(Z)^2}{\sigma_t^2}\right]\|s_t^\theta(X_t) - \left[-\frac{1}{\sigma_t^2}(X_t - \alpha_t Z)\right]\|^2
\]
\[
\stackrel{(ii)}{=} \mathbb{E}_{t,Z\sim\mathcal{N},X_t\sim p_{t}}\left[\frac{(Z)^2}{\sigma_t^2}\right]\|s_t^\theta(X_t) - \nabla \log p_{t+1}(X_t|Z)\|^2,
\]
其中在（i）中，我们通过\( s_t^\theta = x_{0|t}^\theta / \sigma_t \)对神经网络进行了重参数化，在（ii）中我们使用了方程（4.71）。上述损失也被称为去噪得分匹配（Vincent，2011）损失，是训练扩散模型所采用的基本损失函数。根据定理7，我们得到极小化器\( \theta^* \)满足：
\[
s_t^{\theta^*}(x) = -\frac{1}{\sigma_t}\mathbb{E}[X_0|X_t = x] \stackrel{(4.77)}{=} \nabla \log p_t(x). \tag{10.10}
\]

即，在损失最小时，\( s_t^\theta \) 等于边缘概率路径的得分函数 \( \nabla \log p(x) \)。因此，网络 \( s_t^\theta \) 也被称为得分网络。因此，我们可以总结如下：
扩散模型的训练算法等同于对一个采用 \( x_0 \) 预测的特定流模型（FM）进行训练。具体而言，除了重参数化之外，它与训练一个具有以下特点的流模型是相同的：
（1）**概率路径**：使用通过具有仿射漂移系数（\( \alpha_t, \sigma_t \) 由式（10.7）定义）的随机微分方程（SDE）构建的、具有独立耦合的高斯概率路径。
（2）**得分参数化**：通过得分函数对边缘速度场进行重参数化。

在表1中，我们列出了如何轻松地将得分网络转换为其他速度参数化形式。因此，不同的参数化在理论上是等价的，甚至可以在训练后交换参数化形式（见4.8.1节）。不过需要注意的是，得分和 \( x_0 \) 预测参数化在时间 \( t = 0 \)（接近噪声）时会引入奇异性。

## 10.4 采样
接下来，我们讨论从扩散模型中采样，以及它与从FM或GM模型中采样的关系。

**常微分方程（ODE）确定性采样**：如果我们将扩散模型视为一个流模型（FM），我们可以通过从边缘向量场采样来实现采样。在（4.78）中，我们通过得分函数（针对高斯路径）表示了边缘向量场：
\[
u_t(x) = \frac{\dot{\alpha}_t}{\alpha_t}x - \left[\dot{\sigma}_t\sigma_t - \sigma_t^2\frac{\dot{\alpha}_t}{\alpha_t}\right]\nabla \log p_t(x). \tag{10.11}
\]
利用式（10.7）中 \( \alpha_t \)、\( \sigma_t \) 的特定形式，可推导出等价恒等式：
\[
u_t(x) = \dot{k}(t)\left[\alpha_t x - \frac{g_t^2}{2}\nabla \log p_t(x)\right]. \tag{10.12}
\]
或者，也可以将上述 \( u_t(x) \) 代入连续性方程来直接验证这一点。与 \( u_t \) 对应的常微分方程也被称为**概率流常微分方程**（Song 等人，2021）：
\[
\mathrm{d}X_t = \dot{k}(t)\left[\alpha_t X_t - \frac{g_t^2}{2}s_t^\theta(X_t)\right]\mathrm{d}t, \tag{10.13}
\]
其中我们将 \( s_t^\theta(x) = \nabla \log p_t(x) \) 设为学习到的得分函数。需要注意的是，我们在这里使用了常用于随机微分方程（SDE）的常微分方程符号，其原因接下来会变得清晰。我们还注意到，与（Song 等人，2021）相比，在式（10.11）中我们由于时间重参数化而添加了项 \( \dot{k}(t) \)。

**随机微分方程（SDE）随机采样**：在9.7节中，我们推导出可以将朗之万动力学
\[
\frac{1}{2}\beta_t^2 \nabla \log p_t(x)dt + \beta_t dW_t \tag{10.14}
\]
添加到任何连续时间马尔可夫过程（CTMP）生成模型中，这样我们会得到一个遵循相同概率路径的生成模型。我们可以将其应用于概率流常微分方程，以得到一整个生成概率路径 \( p_t \) 的随机微分方程族：
\[
dX_t = \left( \dot{k}(t)\alpha_t X_t + \frac{\beta_t^2 - \dot{k}(t)g_t^2}{2}\nabla \log p_t(X_t) \right) dt + \beta_t dW_t. \tag{10.15}
\]

上述内容涉及扩散模型的**随机采样**。从理论上讲，对于所有 \( \beta_t \geq 0 \)，上述所有模型都会产生相同的边缘分布。这是关于基础随机过程真实情况的一个数学事实。在实践中，我们需要用经过训练的网络 \( s_t^\theta \) 来模拟随机微分方程
\[
dX_t = \left( \dot{k}(t)\alpha_t X_t + \frac{\beta_t^2 - \dot{k}(t)g_t^2}{2}s_t^\theta(X_t) \right) dt + \beta_t dW_t \tag{10.16}
\]
（存在估计误差，即 \( s_t^\theta \) 训练不完善，以及模拟误差，即基础随机微分方程采样不完善）。因此，存在一个最优的未知噪声水平 \( \beta_t \)，它可以通过经验确定（例如，见 Karras 等人，2022），也能从理论上确定（见 Ma 等人，2024）（例如，见 Albergo 和 Vanden-Eijnden，2022，方程（2.45））。

因此，我们得到：
1. **常微分方程（ODE）采样**：对于高斯源、独立耦合，按照式（10.7）确定\( \alpha_t \)、\( \sigma_t \)，并采用得分参数化的情况，用概率流常微分方程对扩散模型进行采样，与对一个流模型（FM）进行采样是相同的。
2. **随机微分方程（SDE）采样**：在相同条件下，用随机SDE采样对扩散模型进行采样，等价于对通过式（10.15）定义的生成模型（GM）进行采样。

## 10.5 时间反转和反向过程的作用
为了结束对扩散模型的讨论，我们在扩散模型的背景下讨论时间反转和反向过程的作用。鉴于时间反转的概念对扩散模型如此重要，这对一些人来说可能会感到惊讶，因为流模型（FM）并不需要它。因此，在这里我们更详细地解释这一点。具体而言，为了生成数据，扩散模型将训练构建为学习一个反向过程\( \bar{X}_r \)，该过程从\( 0 \)到\( R > 0 \)进行，使得
\[
\bar{X}_r \stackrel{d}{=} X_{R - r} \quad \text{对所有} \ r \in [0, R] \tag{10.17}
\]
其中\( \stackrel{d}{=} \)表示分布相等。一旦我们找到这样的过程，我们可以用一个高斯分布初始化\( \bar{X}_0 \stackrel{d}{=} X_R \)，并对其进行模拟以得到\( X_R \stackrel{d}{=} X_0 \sim q \)，即从数据分布\( q \)中抽取一个样本。如果\( X_r \sim \bar{p}_r \)（即，它是如式（10.5）中的边缘概率路径），那么式（10.17）等价于：
\[
\bar{X}_r \sim \bar{p}_r \stackrel{d}{=} \bar{p}_{R - r} \quad \text{对所有} \ r \in [0, R].
\]

换句话说，我们希望随机过程\( \bar{X}_r \)生成概率路径\( \bar{p}_r \)。但这正是我们在生成匹配（Generate Matching）中为生成马尔可夫过程所做的（见式（8.10）），在流匹配（Flow Matching）中对于流也是如此。因此，我们得到：

以下问题是等价的：
（1）**时间反转边缘分布**：为反向过程寻找一个随机微分方程（SDE）（或常微分方程（ODE）），使其具有与正向过程相同的边缘分布——正如扩散模型所做的那样。
（2）**生成概率路径**：寻找一个SDE（或ODE），以生成由正向过程定义的概率路径。
（3）**求解福克-普朗克方程（KFE）**：寻找一个SDE（或ODE），以求解福克-普朗克方程（或连续性方程）。

扩散模型的原始描述包含了随机微分方程（SDE）的“完全”时间反转（Anderson，1982）。这是一个比我们所使用的概念更强的概念，也就是说，它要求跨时间点的联合分布相同：
\[
\mathbb{P}[\bar{X}_{r_1} \in A_1, \dots, \bar{X}_{r_n} \in A_n] = \mathbb{P}[X_{R - r_1} \in A_1, \dots, X_{R - r_n} \in A_n] \tag{10.18}
\]

\[对于所有  0 \leq r_1, \dots, r_n  以及  A_1, \dots, A_n \in \mathcal{S} （可测集）都成立。 \tag{10.19}\]

如Anderson（1982）所示，对于式（10.15）中\( \beta_t \)的特定选择，通过反向过程可以得到满足上述条件的时间反转。然而，出于生成建模的目的，我们通常只使用马尔可夫过程的最终点\( X_1 \)（例如，一个生成的图像），而舍弃更早的时间点。因此，马尔可夫过程是否是“真”时间反转（即仅具有相同的边缘分布，如式（10.17）所示），对于许多应用来说并不重要。一个著名的例子是概率流常微分方程（ODE），概率流ODE并不构成Anderson（1982）意义上的扩散模型的时间反转，但它遵循相同的边缘分布。这表明，寻找“真”时间反转是一个更难解决的数学问题，但（通常）对于生成建模的目的来说并非必要。概率流ODE是当前在神经网络计算次数较少的情况下进行采样的最先进方法（Karras等人，2022）。这种“假”时间反转甚至可能比福克-普朗克方程的其他解法给出次优的结果（Ma等人，2024）。


## 10.6 与其他去噪模型的关系
受扩散模型成功的启发，在将扩散模型的方法迁移到其他状态空间方面取得了显著进展（Campbell 等人，2022、2024；De Bortoli 等人，2022；Huang 等人，2022a；Benton 等人，2022）。与扩散模型类似，它们构建了一个正向马尔可夫过程，该过程对数据进行加噪，然后进行时间反转（即“去噪”）。因此，我们非正式地将这类模型称为“去噪模型”。自然地，人们会问这些模型与生成匹配（GM，见第 9 节）有何关联。简而言之，这些“去噪模型”与 GM 模型的关系，和扩散模型与流匹配（FM）的关系是相同的：
（1）**时间约定**：它们采用扩散模型的时间约定，即时间 0 对应数据。
（2）**概率路径构建**：通过“正向”或“加噪”过程来构建概率路径。
（3）**求解福克 - 普朗克方程（KFE）**：通过正向过程的时间反转来找到福克 - 普朗克方程的一个特定解。

我们承认这是一个非正式的规则，可能存在例外情况。因此，我们建议参考（Holderrieth 等人，2024）中对这类相关工作的扩展且详细的讨论（包括更完整的参考文献列表）。